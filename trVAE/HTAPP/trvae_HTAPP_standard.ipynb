{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import torch\n",
    "import scarches as sca\n",
    "from scarches.dataset.trvae.data_handling import remove_sparsity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(dpi=200, frameon=False)\n",
    "sc.set_figure_params(dpi=200)\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "torch.set_printoptions(precision=3, sci_mode=False, edgeitems=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7079191",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata= sc.read('/work/trvae_new/New_fixed_data/HTAPP_997_processed_raw_FINAL.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625daa5-3763-45dc-9e21-b603c89666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the unwanted cell type\n",
    "adata = adata[adata.obs[\"cell_type\"] != \"mature NK T cell\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0466af-75fa-41dd-8263-4e67fdde2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"cell_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8ec31-1de9-4d11-a598-beb4e18d38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, test_ids = train_test_split(adata.obs_names, test_size=0.1, random_state=42)\n",
    "adata.obs[\"split\"] = \"train\"\n",
    "adata.obs.loc[test_ids, \"split\"] = \"test\"\n",
    "\n",
    "train_adata = adata[adata.obs[\"split\"] == \"train\"]\n",
    "test_adata = adata[adata.obs[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a93833-affe-425d-b2bc-9afc39f47728",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adata.obs[\"dummy_condition\"] = \"same_condition\"\n",
    "test_adata.obs[\"dummy_condition\"] = \"same_condition\"\n",
    "adata.obs[\"dummy_condition\"] = \"same_condition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4143752",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_kwargs = {\n",
    "    \"early_stopping_metric\": \"val_unweighted_loss\",\n",
    "    \"threshold\": 0,\n",
    "    \"patience\": 20,\n",
    "    \"reduce_lr\": True,\n",
    "    \"lr_patience\": 13,\n",
    "    \"lr_factor\": 0.1,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2288b5-f033-47c1-aa1f-59e99ef7fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "trvae = sca.models.TRVAE(\n",
    "    adata=train_adata,\n",
    "    condition_key=\"dummy_condition\",\n",
    "    conditions=[\"same_condition\"],  # only one condition\n",
    "    hidden_layer_sizes=[128, 128],\n",
    ")\n",
    "trvae.train(n_epochs=300, alpha_epoch_anneal=200, early_stopping_kwargs=early_stopping_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2be6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trvae.save(\"trvae_new/fixed_models/trvae_HTAPP_raw_model_newconditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35596a15-24ce-4592-87bb-f20a6e8f8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "trvae.load(\"/work/trvae_new/new_model_runs_GPU/trVAE_HTAPP_new\", adata=train_adata, map_location=torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd72860-8fd0-4be1-9385-857ebfbf3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scarches.trainers.trvae._utils import make_dataset, custom_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006706d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trvae(model, adata, condition_key, batch_size=128):\n",
    "    # evaluation mode\n",
    "    model.model.eval()\n",
    "\n",
    "    # Create a dataset and dataloader for prediction\n",
    "    predict_data, _ = make_dataset(\n",
    "        adata,\n",
    "        train_frac=1.0,\n",
    "        condition_key=condition_key,\n",
    "        cell_type_keys=None, \n",
    "        condition_encoder=model.model.condition_encoder,\n",
    "        cell_type_encoder=None, \n",
    "    )\n",
    "    # Create dataloader \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=predict_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # store results\n",
    "    latent_list = []\n",
    "    reconstructed_list = []\n",
    "\n",
    "    device = next(model.model.parameters()).device\n",
    "\n",
    "    # Perform prediction, moves each part of the data that the device the model is trained on \n",
    "    with torch.no_grad():\n",
    "        for batch_iter, batch_data in enumerate(dataloader):\n",
    "            for key, batch in batch_data.items():\n",
    "                batch_data[key] = batch.to(device)\n",
    "            # Get latent\n",
    "            sf = np.ravel(batch_data[\"x\"].sum(1))\n",
    "            sf=torch.tensor(sf,device=batch_data[\"x\"].device)\n",
    "            size_factor_view = sf.unsqueeze(1).expand(batch_data[\"x\"].size(0), batch_data[\"x\"].size(1))\n",
    "            \n",
    "            x_log = torch.log(1 + batch_data[\"x\"])\n",
    "            z1_mean, z1_log_var = model.model.encoder(x_log, batch_data[\"batch\"])\n",
    "            latent = model.model.sampling(z1_mean, z1_log_var)\n",
    "            latent_list.append(latent.cpu().numpy())\n",
    "\n",
    "\n",
    "            # Get recon, NB, takes latent space from encoder and decodes it\n",
    "            outputs = model.model.decoder(latent, batch_data[\"batch\"])\n",
    "            recon_x, _ = outputs\n",
    "\n",
    "            sf_rate = size_factor_view * recon_x\n",
    "\n",
    "\n",
    "            reconstructed_list.append(sf_rate.cpu().numpy())\n",
    "\n",
    "            \n",
    "\n",
    "    latent = np.concatenate(latent_list, axis=0)\n",
    "    reconstructed = np.concatenate(reconstructed_list, axis=0)\n",
    "\n",
    "    return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Gpu run this instead: \n",
    "\n",
    "def predict_trvae(model, adata, condition_key, batch_size=128):\n",
    "    # evaluation mode\n",
    "    model.model.eval()\n",
    "\n",
    "    # Create a dataset and dataloader for prediction\n",
    "    predict_data, _ = make_dataset(\n",
    "        adata,\n",
    "        train_frac=1.0,\n",
    "        condition_key=condition_key,\n",
    "        cell_type_keys=None, \n",
    "        condition_encoder=model.model.condition_encoder,\n",
    "        cell_type_encoder=None, \n",
    "    )\n",
    "    # Create dataloader \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=predict_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # store results\n",
    "    latent_list = []\n",
    "    reconstructed_list = []\n",
    "\n",
    "\n",
    "    # Perform prediction, moves each part of the data that the device the model is trained on \n",
    "    with torch.no_grad():\n",
    "        device = next(model.model.parameters()).device\n",
    "        for batch_data in dataloader:\n",
    "            for k,v in batch_data.items():\n",
    "                batch_data[k] = v.to(device)\n",
    "\n",
    "            # sum across features → shape [batch_size]\n",
    "            sf = batch_data[\"x\"].sum(dim=1)  \n",
    "            # expand into [batch_size, n_genes]\n",
    "            size_factor_view = sf.unsqueeze(1).expand(\n",
    "                batch_data[\"x\"].size(0),\n",
    "                batch_data[\"x\"].size(1)\n",
    "            )\n",
    "\n",
    "            # log‐transform\n",
    "            x_log = torch.log1p(batch_data[\"x\"])\n",
    "            z1_mean, z1_log_var = model.model.encoder(x_log, batch_data[\"batch\"])\n",
    "            latent = model.model.sampling(z1_mean, z1_log_var)\n",
    "            latent_list.append(latent.cpu().numpy())\n",
    "\n",
    "            outputs = model.model.decoder(latent, batch_data[\"batch\"])\n",
    "            recon_x, _ = outputs\n",
    "            sf_rate = size_factor_view * recon_x\n",
    "            reconstructed_list.append(sf_rate.cpu().numpy())\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    latent = np.concatenate(latent_list, axis=0)\n",
    "    reconstructed = np.concatenate(reconstructed_list, axis=0)\n",
    "\n",
    "    return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04824a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, rec = predict_trvae(trvae, test_adata, condition_key=\"dummy_condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61c648-e400-42e1-b265-62ad5db41ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_2,rec_2 = predict_trvae(trvae, adata, condition_key=\"dummy_condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2 = adata[test_adata.obs_names].X\n",
    "\n",
    "# Convert to dense if it's sparse\n",
    "if not isinstance(adata_2, np.ndarray):\n",
    "    print(\"Converting y_true from sparse to dense.\")\n",
    "    adata_2 = adata_2.toarray()\n",
    "\n",
    "\n",
    "\n",
    "# Now flatten\n",
    "adata_2_flat = adata_2.flatten()\n",
    "#rec_2_flat = rec_2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd(), \"Writable?\", os.access(os.getcwd(), os.W_OK))\n",
    "\n",
    "# 1) copy to avoid view‐warning\n",
    "test_adata = test_adata.copy()\n",
    "test_adata.obsm[\"X_reconstructed\"] = rec\n",
    "\n",
    "# 2) write to /tmp (or somewhere you have access)\n",
    "outfn = \"/work/trvae_new/trvae_newpredict/adata_post_with_latent_and_reconstructed_HTAPP_trVAE_RAWasdas.h5ad\"\n",
    "test_adata.write(outfn)\n",
    "print(\"Wrote to\", outfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mutual_info_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 or R2 adj\n",
    "# Flatten arrays it is needed, depends on the dimensionality\n",
    "adata_2_flat = adata_2.flatten()\n",
    "rec_2_flat = rec.flatten()\n",
    "\n",
    "\n",
    "r_square = r2_score(adata_2_flat, rec_2_flat)\n",
    "print(\"R2:\", r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse = mean_squared_error(adata_2, rec)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE \n",
    "\n",
    "mae = mean_absolute_error(adata_2, rec)\n",
    "print(f\"Mean absolute error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "###MIG###\n",
    "import numpy as np\n",
    "import anndata\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical(data):\n",
    "    encoders = []\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "        encoders.append(le)\n",
    "    return encoded_data, encoders\n",
    "\n",
    "def prep_data(adata, embedding, covriate_keys=None):\n",
    "    encoded_factors_of_variation, _ = encode_categorical(adata.obs[covriate_keys].values)\n",
    "\n",
    "    if isinstance(embedding, anndata.AnnData):  \n",
    "        embedding_data = embedding.X\n",
    "    else:\n",
    "        embedding_data = embedding\n",
    "\n",
    "    mus = np.array(embedding_data)\n",
    "    ys = np.array(encoded_factors_of_variation)\n",
    "\n",
    "    return mus.T.copy(), ys.T.copy()\n",
    "\n",
    "def compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes the mutual information gap.\"\"\"\n",
    "    return _compute_mig(mus, ys, covariate_names)\n",
    "\n",
    "def _compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes MIG score based on latent codes and covariates.\"\"\"\n",
    "    score_dict = {}\n",
    "    discretized_mus = make_discretizer(mus, discretizer_fn=_histogram_discretize)\n",
    "   # print(\"Sample Discretized Latent Variables:\\n\", discretized_mus[:, :5])\n",
    "    m = discrete_mutual_info(discretized_mus, ys)\n",
    "\n",
    "    if covariate_names is None:\n",
    "        covariate_names = [f\"Covariate {j}\" for j in range(m.shape[1])]\n",
    "        \n",
    "    for j in range(m.shape[1]):\n",
    "        top_indices = np.argsort(m[:, j])[::-1][:3]\n",
    "        top_scores = m[top_indices, j]\n",
    "        print(f\"Top 3 MI scores for covariate '{covariate_names[j]}':\")\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            print(f\"  Latent dim {idx}: MI = {score:.4f}\")\n",
    "\n",
    "    assert m.shape[0] == mus.shape[0]\n",
    "    assert m.shape[1] == ys.shape[0]\n",
    "\n",
    "    entropy = discrete_entropy(ys)\n",
    "    sorted_m = np.sort(m, axis=0)[::-1]\n",
    "\n",
    "    score_dict[\"discrete_mig\"] = np.mean(\n",
    "        np.divide(sorted_m[0, :] - sorted_m[1, :], entropy[:])\n",
    "    )\n",
    "\n",
    "    print(\"Þetta er score:\", score_dict)\n",
    "    print(\"Entropy values:\", entropy)\n",
    "    return score_dict\n",
    "\n",
    "def discrete_mutual_info(mus, ys):\n",
    "    num_codes = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    m = np.zeros([num_codes, num_factors])\n",
    "    \n",
    "    for i in range(num_codes):\n",
    "        for j in range(num_factors):\n",
    "            m[i, j] = mutual_info_score(ys[j, :], mus[i, :])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def discrete_entropy(ys):\n",
    "    num_factors = ys.shape[0]\n",
    "    h = np.zeros(num_factors)\n",
    "    \n",
    "    for j in range(num_factors):\n",
    "        h[j] = mutual_info_score(ys[j, :], ys[j, :])\n",
    "    \n",
    "    return h\n",
    "\n",
    "def _identity_discretizer(target, num_bins):\n",
    "    del num_bins\n",
    "    return target\n",
    "\n",
    "\n",
    "def _histogram_discretize(target, num_bins=10):\n",
    "    discretized = np.zeros_like(target)\n",
    "    for i in range(target.shape[0]):\n",
    "        discretized[i, :] = np.digitize(target[i, :], np.histogram(\n",
    "            target[i, :], num_bins)[1][:-1])\n",
    "    return discretized\n",
    "\n",
    "\n",
    "def make_discretizer(target, num_bins=10, discretizer_fn=_histogram_discretize):\n",
    "    return discretizer_fn(target, num_bins)\n",
    "\n",
    "\n",
    "def score_disentanglement(adata, embedding_data, embedding_basal, covriate_keys=None, continuous_covriate_keys=None):\n",
    "    mus, ys = prep_data(adata, embedding_data, covriate_keys=covriate_keys)\n",
    "    print('Computing MIG')\n",
    "    mig = compute_mig(mus, ys, covariate_names=covriate_keys)\n",
    "    return mig, mus, ys\n",
    "\n",
    "# Run MIG score\n",
    "mig_1 = score_disentanglement(\n",
    "    adata,\n",
    "    latent_2,\n",
    "    None,\n",
    "    covriate_keys=[\"cnv_pass_mal\", \"compartments\",\"Phase\", \"replicate\", \"cell_type\"]\n",
    ")\n",
    "\n",
    "print(\"MIG Score:\", mig_1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe16168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized DCI computation based on disentanglement_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Encoding and Preprocessing ===\n",
    "def encode_categorical(data):\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "    return encoded_data\n",
    "\n",
    "def remove_duplicate_columns(df):\n",
    "    df_unique = df.T.drop_duplicates().T\n",
    "    return df_unique\n",
    "\n",
    "def prep_data(adata, embedding, covariate_keys, test_size=0.25):\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        range(len(adata)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    cov_df = adata.obs[covariate_keys].copy()\n",
    "    cov_df = remove_duplicate_columns(cov_df)\n",
    "    encoded_factors = encode_categorical(cov_df.values)\n",
    "    embedding_data = embedding.X if isinstance(embedding, anndata.AnnData) else embedding\n",
    "    mus_train = embedding_data[idx_train]\n",
    "    mus_test = embedding_data[idx_test]\n",
    "    ys_train = encoded_factors[idx_train]\n",
    "    ys_test = encoded_factors[idx_test]\n",
    "    return mus_train.T, ys_train.T, mus_test.T, ys_test.T\n",
    "\n",
    "# === Importance Matrix ===\n",
    "def compute_importance_rf(x_train, y_train, x_test, y_test):\n",
    "    num_factors = y_train.shape[0]\n",
    "    num_codes = x_train.shape[0]\n",
    "    importance_matrix = np.zeros((num_codes, num_factors))\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(num_factors):\n",
    "        model = RandomForestClassifier(random_state=42, max_depth=5)\n",
    "        model.fit(x_train.T, y_train[i])\n",
    "        importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "        train_acc.append(np.mean(model.predict(x_train.T) == y_train[i]))\n",
    "        test_acc.append(np.mean(model.predict(x_test.T) == y_test[i]))\n",
    "    return importance_matrix, np.mean(train_acc), np.mean(test_acc)\n",
    "\n",
    "# === Disentanglement ===\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "    row_sums = importance_matrix.sum(axis=1, keepdims=True)\n",
    "    safe_matrix = np.where(row_sums == 0, 1e-11, row_sums)\n",
    "    normalized = importance_matrix / safe_matrix\n",
    "    return 1. - entropy(normalized.T + 1e-11, base=importance_matrix.shape[1])\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "    per_code = disentanglement_per_code(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    code_importance = importance_matrix.sum(axis=1) / total\n",
    "    return np.sum(per_code * code_importance)\n",
    "\n",
    "# === Completeness ===\n",
    "def completeness_per_factor(importance_matrix):\n",
    "    return 1. - entropy(importance_matrix + 1e-11, base=importance_matrix.shape[0])\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "    per_factor = completeness_per_factor(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    factor_importance = importance_matrix.sum(axis=0) / total\n",
    "    return np.sum(per_factor * factor_importance)\n",
    "\n",
    "# === DCI Master Function ===\n",
    "def compute_dci(mus_train, ys_train, mus_test, ys_test):\n",
    "    importance_matrix, train_acc, test_acc = compute_importance_rf(\n",
    "        mus_train, ys_train, mus_test, ys_test\n",
    "    )\n",
    "    threshold = 1e-11\n",
    "    importance_matrix = np.where(importance_matrix < threshold, 0, importance_matrix)\n",
    "    return {\n",
    "        \"disentanglement\": disentanglement(importance_matrix),\n",
    "        \"completeness\": completeness(importance_matrix),\n",
    "        \"informativeness_train\": train_acc,\n",
    "        \"informativeness_test\": test_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac09b1b-33b6-4a61-a1fe-4e66414830fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_keys = [\"cnv_pass_mal\", \"compartments\",\"Phase\", \"replicate\", \"cell_type\"]\n",
    "mus_train, ys_train, mus_test, ys_test = prep_data(\n",
    "    adata, latent_2,covariate_keys=covariate_keys )\n",
    "dci_scores = compute_dci(mus_train, ys_train, mus_test, ys_test)\n",
    "dci_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecd5a3-52d1-4003-8f41-1856a1b62be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAP score\n",
    "from sklearn import svm\n",
    "\n",
    "def compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes the SAP score.\n",
    "\n",
    "    Args:\n",
    "        mus, ys, mus_test, ys_test\n",
    "        continuous_factors: Factors are continuous variable (True) or not (False).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with SAP score.\n",
    "    \"\"\"\n",
    "\n",
    "    return _compute_sap(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "\n",
    "def _compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "    score_matrix = compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "    # Score matrix should have shape [num_latents, num_factors].\n",
    "    assert score_matrix.shape[0] == mus.shape[0]\n",
    "    assert score_matrix.shape[1] == ys.shape[0]\n",
    "    scores_dict = {}\n",
    "    scores_dict[\"SAP_score\"] = compute_avg_diff_top_two(score_matrix)\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "def compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Compute score matrix as described in Section 3.\"\"\"\n",
    "    num_latents = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    score_matrix = np.zeros([num_latents, num_factors])\n",
    "    for i in range(num_latents):\n",
    "        for j in range(num_factors):\n",
    "            mu_i = mus[i, :]\n",
    "            y_j = ys[j, :]\n",
    "            if continuous_factors:\n",
    "                # Attribute is considered continuous.\n",
    "                cov_mu_i_y_j = np.cov(mu_i, y_j, ddof=1)\n",
    "                cov_mu_y = cov_mu_i_y_j[0, 1]**2\n",
    "                var_mu = cov_mu_i_y_j[0, 0]\n",
    "                var_y = cov_mu_i_y_j[1, 1]\n",
    "                if var_mu > 1e-12:\n",
    "                    score_matrix[i, j] = cov_mu_y * 1. / (var_mu * var_y)\n",
    "                else:\n",
    "                    score_matrix[i, j] = 0.\n",
    "            else:\n",
    "                # Attribute is considered discrete.\n",
    "                mu_i_test = mus_test[i, :]\n",
    "                y_j_test = ys_test[j, :]\n",
    "                classifier = svm.LinearSVC(C=0.01, class_weight=\"balanced\")\n",
    "                classifier.fit(mu_i[:, np.newaxis], y_j)\n",
    "                pred = classifier.predict(mu_i_test[:, np.newaxis])\n",
    "                score_matrix[i, j] = np.mean(pred == y_j_test)\n",
    "    return score_matrix\n",
    "\n",
    "def compute_avg_diff_top_two(matrix):\n",
    "    sorted_matrix = np.sort(matrix, axis=0)\n",
    "    return np.mean(sorted_matrix[-1, :] - sorted_matrix[-2, :])\n",
    "\n",
    "sap = compute_sap(mus_train, ys_train, mus_test, ys_test, continuous_factors=False)\n",
    "sap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83669d57-81b3-4662-8806-52dfbd1bd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRS \n",
    "\n",
    "\n",
    "def compute_irs(mus, ys, diff_quantile=0.99):\n",
    "    ys_discrete = make_discretizer(ys)\n",
    "\n",
    "    active_mask = (mus.var(axis=1) > 0)\n",
    "    active_mus = mus[active_mask, :]\n",
    "\n",
    "    if active_mus.size == 0:\n",
    "        irs_score = 0.0\n",
    "    else:\n",
    "        irs_score = scalable_disentanglement_score(ys_discrete.T, active_mus.T, diff_quantile)[\"avg_score\"]\n",
    "\n",
    "    score_dict = {}\n",
    "    score_dict[\"IRS\"] = irs_score\n",
    "    score_dict[\"num_active_dims\"] = int(np.sum(active_mask))\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def _drop_constant_dims(ys):\n",
    "    \"\"\"Returns a view of the matrix `ys` with dropped constant rows.\"\"\"\n",
    "    ys = np.asarray(ys)\n",
    "    if ys.ndim != 2:\n",
    "        raise ValueError(\"Expecting a matrix.\")\n",
    "\n",
    "    variances = ys.var(axis=1)\n",
    "    active_mask = variances > 0.\n",
    "    return ys[active_mask, :]\n",
    "\n",
    "\n",
    "def scalable_disentanglement_score(gen_factors, latents, diff_quantile=0.99):\n",
    "    \"\"\"Computes IRS scores of a dataset.\n",
    "\n",
    "    Assumes no noise in X and crossed generative factors (i.e. one sample per\n",
    "    combination of gen_factors). Assumes each g_i is an equally probable\n",
    "    realization of g_i and all g_i are independent.\n",
    "\n",
    "    Args:\n",
    "        gen_factors: Numpy array of shape (num samples, num generative factors),\n",
    "            matrix of ground truth generative factors.\n",
    "        latents: Numpy array of shape (num samples, num latent dimensions), matrix\n",
    "            of latent variables.\n",
    "        diff_quantile: Float value between 0 and 1 to decide what quantile of diffs\n",
    "            to select (use 1.0 for the version in the paper).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with IRS scores.\n",
    "    \"\"\"\n",
    "    num_gen = gen_factors.shape[1]\n",
    "    num_lat = latents.shape[1]\n",
    "\n",
    "    # Compute normalizer.\n",
    "    max_deviations = np.max(np.abs(latents - latents.mean(axis=0)), axis=0)\n",
    "    cum_deviations = np.zeros([num_lat, num_gen])\n",
    "    for i in range(num_gen):\n",
    "        unique_factors = np.unique(gen_factors[:, i], axis=0)\n",
    "        assert unique_factors.ndim == 1\n",
    "        num_distinct_factors = unique_factors.shape[0]\n",
    "        for k in range(num_distinct_factors):\n",
    "            # Compute E[Z | g_i].\n",
    "            match = gen_factors[:, i] == unique_factors[k]\n",
    "            e_loc = np.mean(latents[match, :], axis=0)\n",
    "\n",
    "            # Difference of each value within that group of constant g_i to its mean.\n",
    "            diffs = np.abs(latents[match, :] - e_loc)\n",
    "            max_diffs = np.percentile(diffs, q=diff_quantile*100, axis=0)\n",
    "            cum_deviations[:, i] += max_diffs\n",
    "        cum_deviations[:, i] /= num_distinct_factors\n",
    "    # Normalize value of each latent dimension with its maximal deviation.\n",
    "    normalized_deviations = cum_deviations / max_deviations[:, np.newaxis]\n",
    "    irs_matrix = 1.0 - normalized_deviations\n",
    "    disentanglement_scores = irs_matrix.max(axis=1)\n",
    "    if np.sum(max_deviations) > 0.0:\n",
    "        avg_score = np.average(disentanglement_scores, weights=max_deviations)\n",
    "    else:\n",
    "        avg_score = np.mean(disentanglement_scores)\n",
    "\n",
    "    parents = irs_matrix.argmax(axis=1)\n",
    "    score_dict = {}\n",
    "    score_dict[\"disentanglement_scores\"] = disentanglement_scores\n",
    "    score_dict[\"avg_score\"] = avg_score\n",
    "    score_dict[\"parents\"] = parents\n",
    "    score_dict[\"IRS_matrix\"] = irs_matrix\n",
    "    score_dict[\"max_deviations\"] = max_deviations\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "\n",
    "irs = compute_irs(mus_train, ys_train, diff_quantile=0.99)\n",
    "irs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f26dec-24ec-4e33-9e23-eff524979bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
