{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf21d52-eed4-4c3f-bc55-3da0cdb4a410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/theislab/cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e417bf-6320-4d4b-bea0-3cc8e5294438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cpa\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca2c07-a3b6-4223-9fae-9c0be54d9cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e78781-f4e1-4b24-a682-3819d33be088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read(\"/work/CPA_Healthy_hamstring/new_data_raw_fix/HTAPP_997_processed_raw_FINAL.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77627b9c-a7f5-49c9-bd8c-cde3937034a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36562763-311e-4506-89ba-337c5d7ab23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data set into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_key = \"split\"\n",
    "adata.obs[split_key] = \"train\"\n",
    "idx = list(range(len(adata)))\n",
    "idx_train, idx_test = train_test_split(adata.obs_names, test_size=0.1, random_state=42)\n",
    "adata.obs.loc[idx_train, split_key] = \"train\"\n",
    "adata.obs.loc[idx_test, split_key] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d96ad9-ba47-47a7-9646-017608fdf198",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train = adata[adata.obs[split_key] == \"train\"].copy()\n",
    "adata_test = adata[adata.obs[split_key] == \"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24706c18-dcb7-482b-a1f7-da6e8b9ee87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb60f6c-dd1d-4e4d-aaba-4dd5299c0889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642bbed-9a8a-4e97-b4b7-9d9cf300c3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.layers['counts'] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30600d7-8353-423d-9ecd-4d8e85836c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.X = adata.layers['counts'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f265497-e728-4f11-80e3-8106f067f147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpa.CPA.setup_anndata(adata,\n",
    "                      perturbation_key='cell_type',\n",
    "                      dosage_key=None,\n",
    "                      control_group='neuron',\n",
    "                      batch_key=None,\n",
    "                      is_count_data=True,\n",
    "                      categorical_covariate_keys=['cell_type', 'Phase', 'replicate', 'compartments' , 'cnv_pass_mal'],\n",
    "                      deg_uns_key=None,\n",
    "                      deg_uns_cat_key=None,\n",
    "                      max_comb_len=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab231f5-16bf-4d71-bf3c-c76c7cf7a03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_latent\": 64,\n",
    "    \"recon_loss\": \"nb\",\n",
    "    \"doser_type\": \"linear\",\n",
    "    \"n_hidden_encoder\": 128,\n",
    "    \"n_layers_encoder\": 2,\n",
    "    \"n_hidden_decoder\": 512,\n",
    "    \"n_layers_decoder\": 2,\n",
    "    \"use_batch_norm_encoder\": True,\n",
    "    \"use_layer_norm_encoder\": False,\n",
    "    \"use_batch_norm_decoder\": False,\n",
    "    \"use_layer_norm_decoder\": True,\n",
    "    \"dropout_rate_encoder\": 0.0,\n",
    "    \"dropout_rate_decoder\": 0.1,\n",
    "    \"variational\": False,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "trainer_params = {\n",
    "    \"n_epochs_kl_warmup\": None,\n",
    "    \"n_epochs_pretrain_ae\": 30,\n",
    "    \"n_epochs_adv_warmup\": 50,\n",
    "    \"n_epochs_mixup_warmup\": 0,\n",
    "    \"mixup_alpha\": 0.0,\n",
    "    \"adv_steps\": None,\n",
    "    \"n_hidden_adv\": 64,\n",
    "    \"n_layers_adv\": 3,\n",
    "    \"use_batch_norm_adv\": True,\n",
    "    \"use_layer_norm_adv\": False,\n",
    "    \"dropout_rate_adv\": 0.3,\n",
    "    \"reg_adv\": 20.0,\n",
    "    \"pen_adv\": 5.0,\n",
    "    \"lr\": 0.0003,\n",
    "    \"wd\": 4e-07,\n",
    "    \"adv_lr\": 0.0003,\n",
    "    \"adv_wd\": 4e-07,\n",
    "    \"adv_loss\": \"cce\",\n",
    "    \"doser_lr\": 0.0003,\n",
    "    \"doser_wd\": 4e-07,\n",
    "    \"do_clip_grad\": True,\n",
    "    \"gradient_clip_value\": 1.0,\n",
    "    \"step_size_lr\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7ef61-0428-425a-94aa-a55e5bac6e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = cpa.CPA(adata=adata,\n",
    "                split_key='split',\n",
    "                train_split='train',\n",
    "                valid_split='test',\n",
    "                **model_params,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fca6b-65d4-4a1d-8cc8-ecd3de3c3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(max_epochs=500,\n",
    "            use_gpu=True,\n",
    "            batch_size=512,\n",
    "            plan_kwargs=trainer_params,\n",
    "            early_stopping_patience=10,\n",
    "            check_val_every_n_epoch=500,\n",
    "            save_path='HTAP_CPA/new_fixed_model_raw_HTAPP',\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff329ba-8f87-48e6-a987-cefcb058a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/HTAP_CPA(1)/HTAP_CPA/Results/history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92295983-b373-42c0-8bfe-ae002d06457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(df,x=\"epoch\", y= \"r2_mean\", hue=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790186b-726b-42a6-bfa2-70017a4faab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Assuming 'adata' is your AnnData object\n",
    "# Extract the CPA_pred from the obsm attribute\n",
    "adata_CPA_pred = adata.copy()  # Make a copy to avoid modifying the original object\n",
    "adata_CPA_pred.obsm['CPA_pred'] = adata.obsm['CPA_pred']  # Ensure the CPA_pred is included\n",
    "\n",
    "# Save the new AnnData object to a new .h5ad file\n",
    "adata_CPA_pred.write('adata_with_CPA_pred_HTAPP.h5ad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d523f53-be28-4b49-b782-db279081cf56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model.load(\"/work/HTAP_CPA(1)/HTAP_CPA(1)/Results/\", adata=adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870c1c5-5432-4cca-af3f-32fdfc6b0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa.pl.plot_history(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d29e43-7ebd-45e9-9b8c-1a5424456d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa.CPA.setup_anndata(adata_test,\n",
    "                      perturbation_key='cell_type',\n",
    "                      dosage_key=None,\n",
    "                      control_group='neuron',\n",
    "                      batch_key=None,\n",
    "                      is_count_data=True,\n",
    "                      categorical_covariate_keys=['cell_type', 'Phase', 'replicate', 'compartments' , 'cnv_pass_mal'],\n",
    "                      deg_uns_key=None,\n",
    "                      deg_uns_cat_key=None,\n",
    "                      max_comb_len=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa9471-548b-4c7b-8b8a-46c5997a8baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_outputs = model.get_latent_representation(adata, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54991f1-dd02-4bf7-b1cd-53857fd06aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.verbosity = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e396ae0-ff63-4cb7-b61e-d807b2c2b530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_basal_adata = latent_outputs['latent_basal']\n",
    "latent_adata = latent_outputs['latent_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8728e1-0fe2-454c-80d2-7ae084956dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37322a5e-8643-4a77-880e-448343669981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(latent_basal_adata)\n",
    "sc.tl.umap(latent_basal_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc1b3c-7012-433b-b022-8f08c21e3af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_basal_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323883b-c085-4aba-975f-bc16656f33d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(latent_basal_adata, color=['cell_type'], frameon=False, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2d617-7717-45ee-96c1-80cf464e6628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(latent_adata)\n",
    "sc.tl.umap(latent_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e8b6b-cf5b-4fda-b856-336a849a050b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(latent_adata, color=['cell_type'], frameon=False, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e431e5-4e20-4a52-bd44-e13a3ac8ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.read(\"/work/CPA_Healthy_hamstring/adata_HH_with_CPA_pred_new_raw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06680c6-0e05-4dd8-9e3d-37101b222214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.layers['X_true'] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a816d4-980b-4374-965d-e93b9bd36ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(adata_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fafb5-7526-4faf-a901-3dd178ee0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = adata_test.obsm[\"CPA_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b2d92-1cf3-446f-83fe-164e11cc1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439d4e1-f053-4fbf-8767-717b8bb644d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get ground truth from test set\n",
    "X_true = adata_test.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c256816-597d-4772-9550-f08a9e0192c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert sparse matrix to dense if needed\n",
    "if hasattr(X_true, \"toarray\"):\n",
    "    X_true = X_true.toarray()\n",
    "\n",
    "# Step 3: Flatten both arrays\n",
    "X_true_flat = X_true.flatten()\n",
    "rec_flat = rec.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb608d-f87a-48bc-91e5-70024e05c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute R²\n",
    "r2 = r2_score(X_true_flat, rec_flat)\n",
    "print(f\"R² score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b5391-c1ec-49db-93dc-99f7be991c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(X_true_flat, rec_flat)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a052408-5b1b-42cf-ab6f-325d656603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(X_true_flat, rec_flat)\n",
    "print(f\"Mean absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c9b81-7f52-41b8-8d24-3eab69c1d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60614982-c865-4aed-b7cb-8150f5898996",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb09cd4-c384-44c2-bcd7-1ab87a2d6764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceeb91c-8170-4cb7-ac4b-a73d3270b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Assuming 'adata' is your AnnData object\n",
    "# Extract the CPA_pred from the obsm attribute\n",
    "adata_CPA_pred = adata_test.copy()  # Make a copy to avoid modifying the original object\n",
    "adata_CPA_pred.obsm['CPA_pred'] = adata_test.obsm['CPA_pred']  # Ensure the CPA_pred is included\n",
    "\n",
    "# Save the new AnnData object to a new .h5ad file\n",
    "adata_CPA_pred.write('adata_HTAPP_with_CPA_pred_new_raw.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0fe63-cf32-4039-8a07-18d143cf9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_CPA_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ccd1b-e7d6-4f21-b04f-f3166e7200c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import anndata\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical(data):\n",
    "    encoders = []\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "        encoders.append(le)\n",
    "    return encoded_data, encoders\n",
    "\n",
    "def prep_data(adata, embedding, covriate_keys=None, continuous_covriate_keys=None, test_size=0.25):\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        range(len(adata.obs_names)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    print(\"Splitting complete.\")\n",
    "    \n",
    "    encoded_factors_of_variation, _ = encode_categorical(adata.obs[covriate_keys].values)\n",
    "  #  print(\"Encoded factors of variation:\", np.unique(encoded_factors_of_variation, axis=0))\n",
    "   # print(\"Encoded factors of variation sample:\", encoded_factors_of_variation[:5])\n",
    "   # print(\"Categorical encoding complete.\")\n",
    "    \n",
    "    if isinstance(embedding, anndata.AnnData):  \n",
    "        embedding_data = embedding.X\n",
    "    else:\n",
    "        embedding_data = embedding\n",
    "\n",
    "   # print(\"Embedding shape:\", embedding_data.shape)\n",
    "  #  print(\"Number of train indices:\", len(idx_train))\n",
    "   # print(\"Number of test indices:\", len(idx_test))\n",
    "    \n",
    "    mus_train = np.array(embedding_data[idx_train])\n",
    "    ys_train = np.array(encoded_factors_of_variation[idx_train])\n",
    "    mus_test = np.array(embedding_data[idx_test])\n",
    "    ys_test = np.array(encoded_factors_of_variation[idx_test])\n",
    "    \n",
    "   # print(\"mus_train shape:\", mus_train.shape)\n",
    "   # print(\"ys_train shape:\", ys_train.shape)\n",
    "   # print(\"mus_test shape:\", mus_test.shape)\n",
    "   # print(\"ys_test shape:\", ys_test.shape)\n",
    "   # print(\"Sample of mus_train:\", mus_train[:, :5])\n",
    "   # print(\"Sample of ys_train:\", ys_train[:, :5])\n",
    "    #print(\"Min/Max of mus_train:\", mus_train.min(), mus_train.max())\n",
    "   # print(\"Unique values in ys_train:\", np.unique(ys_train))\n",
    "\n",
    "    return mus_train.T.copy(), ys_train.T.copy(), mus_test.T.copy(), ys_test.T.copy()\n",
    "\n",
    "def compute_mig(mus_train, ys_train, covariate_names=None):\n",
    "    \"\"\"Computes the mutual information gap.\"\"\"\n",
    "    return _compute_mig(mus_train, ys_train, covariate_names)\n",
    "\n",
    "def _compute_mig(mus_train, ys_train, covariate_names=None):\n",
    "    \"\"\"Computes MIG score based on latent codes and covariates.\"\"\"\n",
    "    score_dict = {}\n",
    "    discretized_mus = make_discretizer(mus_train, discretizer_fn=_histogram_discretize)\n",
    "   # print(\"Sample Discretized Latent Variables:\\n\", discretized_mus[:, :5])\n",
    "    \n",
    "    m = discrete_mutual_info(discretized_mus, ys_train)\n",
    "\n",
    "    if covariate_names is None:\n",
    "        covariate_names = [f\"Covariate {j}\" for j in range(m.shape[1])]\n",
    "        \n",
    "    for j in range(m.shape[1]):\n",
    "        top_indices = np.argsort(m[:, j])[::-1][:3]\n",
    "        top_scores = m[top_indices, j]\n",
    "        print(f\"Top 3 MI scores for covariate '{covariate_names[j]}':\")\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            print(f\"  Latent dim {idx}: MI = {score:.4f}\")\n",
    "\n",
    "    assert m.shape[0] == mus_train.shape[0]\n",
    "    assert m.shape[1] == ys_train.shape[0]\n",
    "\n",
    "    entropy = discrete_entropy(ys_train)\n",
    "    sorted_m = np.sort(m, axis=0)[::-1]\n",
    "\n",
    "    score_dict[\"discrete_mig\"] = np.mean(\n",
    "        np.divide(sorted_m[0, :] - sorted_m[1, :], entropy[:])\n",
    "    )\n",
    "\n",
    "    print(\"Þetta er score:\", score_dict)\n",
    "    print(\"Entropy values:\", entropy)\n",
    "    return score_dict\n",
    "\n",
    "def discrete_mutual_info(mus, ys):\n",
    "    num_codes = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    m = np.zeros([num_codes, num_factors])\n",
    "    \n",
    "    for i in range(num_codes):\n",
    "        for j in range(num_factors):\n",
    "            m[i, j] = mutual_info_score(ys[j, :], mus[i, :])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def discrete_entropy(ys):\n",
    "    num_factors = ys.shape[0]\n",
    "    h = np.zeros(num_factors)\n",
    "    \n",
    "    for j in range(num_factors):\n",
    "        h[j] = mutual_info_score(ys[j, :], ys[j, :])\n",
    "    \n",
    "    return h\n",
    "\n",
    "def _identity_discretizer(target, num_bins):\n",
    "    del num_bins\n",
    "    return target\n",
    "\n",
    "def make_discretizer(target, num_bins=10, discretizer_fn=_identity_discretizer):\n",
    "    return discretizer_fn(target, num_bins)\n",
    "\n",
    "def _histogram_discretize(target, num_bins=10):\n",
    "    discretized = np.zeros_like(target)\n",
    "    for i in range(target.shape[0]):\n",
    "        discretized[i, :] = np.digitize(target[i, :], np.histogram(\n",
    "            target[i, :], num_bins)[1][:-1])\n",
    "    return discretized\n",
    "\n",
    "def k_means_discretize(target, num_clusters=10):\n",
    "    discretized = np.zeros_like(target)\n",
    "    for i in range(target.shape[0]):\n",
    "        latent_variable = target[i, :].reshape(-1,1)\n",
    "        kmeans = KMeans(n_clusters = num_clusters, random_state=0)\n",
    "        kmeans.fit(latent_variable)\n",
    "        discretized[i,:]=kmeans.labels_\n",
    "    return discretized\n",
    "\n",
    "def score_disentanglement(adata, embedding_data, embedding_basal, covriate_keys=None, continuous_covriate_keys=None, test_size=0.25):\n",
    "    mus_train, ys_train, mus_test, ys_test = prep_data(adata, embedding_data, covriate_keys=covriate_keys)\n",
    "    print('Computing MIG')\n",
    "    mig = compute_mig(mus_train, ys_train, covariate_names=covriate_keys)\n",
    "    return mig\n",
    "\n",
    "# Run MIG score\n",
    "mig_1 = score_disentanglement(\n",
    "    adata,\n",
    "    latent_adata,\n",
    "    None,\n",
    "    covriate_keys=['cell_type', 'Phase', 'replicate', 'compartments' , 'cnv_pass_mal']\n",
    ")\n",
    "\n",
    "print(\"MIG Score:\", mig_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db303f2c-9ced-4133-9e9a-665b5f1a5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6508d-35f7-43fc-9bc5-abb3fedd89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized DCI computation based on disentanglement_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Encoding and Preprocessing ===\n",
    "def encode_categorical(data):\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "    return encoded_data\n",
    "\n",
    "def remove_duplicate_columns(df):\n",
    "    df_unique = df.T.drop_duplicates().T\n",
    "    return df_unique\n",
    "\n",
    "def prep_data(adata, embedding, covariate_keys, test_size=0.25):\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        range(len(adata)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    cov_df = adata.obs[covariate_keys].copy()\n",
    "    cov_df = remove_duplicate_columns(cov_df)\n",
    "    encoded_factors = encode_categorical(cov_df.values)\n",
    "    embedding_data = embedding.X if isinstance(embedding, anndata.AnnData) else embedding\n",
    "    mus_train = embedding_data[idx_train]\n",
    "    mus_test = embedding_data[idx_test]\n",
    "    ys_train = encoded_factors[idx_train]\n",
    "    ys_test = encoded_factors[idx_test]\n",
    "    return mus_train.T, ys_train.T, mus_test.T, ys_test.T\n",
    "\n",
    "# === Importance Matrix ===\n",
    "def compute_importance_rf(x_train, y_train, x_test, y_test):\n",
    "    num_factors = y_train.shape[0]\n",
    "    num_codes = x_train.shape[0]\n",
    "    importance_matrix = np.zeros((num_codes, num_factors))\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(num_factors):\n",
    "        model = RandomForestClassifier(random_state=42, max_depth=5)\n",
    "        model.fit(x_train.T, y_train[i])\n",
    "        importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "        train_acc.append(np.mean(model.predict(x_train.T) == y_train[i]))\n",
    "        test_acc.append(np.mean(model.predict(x_test.T) == y_test[i]))\n",
    "    return importance_matrix, np.mean(train_acc), np.mean(test_acc)\n",
    "\n",
    "# === Disentanglement ===\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "    row_sums = importance_matrix.sum(axis=1, keepdims=True)\n",
    "    safe_matrix = np.where(row_sums == 0, 1e-11, row_sums)\n",
    "    normalized = importance_matrix / safe_matrix\n",
    "    return 1. - entropy(normalized.T + 1e-11, base=importance_matrix.shape[1])\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "    per_code = disentanglement_per_code(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    code_importance = importance_matrix.sum(axis=1) / total\n",
    "    return np.sum(per_code * code_importance)\n",
    "\n",
    "# === Completeness ===\n",
    "def completeness_per_factor(importance_matrix):\n",
    "    return 1. - entropy(importance_matrix + 1e-11, base=importance_matrix.shape[0])\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "    per_factor = completeness_per_factor(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    factor_importance = importance_matrix.sum(axis=0) / total\n",
    "    return np.sum(per_factor * factor_importance)\n",
    "\n",
    "# === DCI Master Function ===\n",
    "def compute_dci(mus_train, ys_train, mus_test, ys_test):\n",
    "    importance_matrix, train_acc, test_acc = compute_importance_rf(\n",
    "        mus_train, ys_train, mus_test, ys_test\n",
    "    )\n",
    "    threshold = 1e-11\n",
    "    importance_matrix = np.where(importance_matrix < threshold, 0, importance_matrix)\n",
    "    return {\n",
    "        \"disentanglement\": disentanglement(importance_matrix),\n",
    "        \"completeness\": completeness(importance_matrix),\n",
    "        \"informativeness_train\": train_acc,\n",
    "        \"informativeness_test\": test_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6904a-320d-411e-9c5c-fb0dca985c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_keys = ['cell_type', 'Phase', 'replicate', 'compartments' , 'cnv_pass_mal']\n",
    "mus_train, ys_train, mus_test, ys_test = prep_data(\n",
    "    adata, latent_adata.X,covariate_keys=covariate_keys )\n",
    "dci_scores = compute_dci(mus_train, ys_train, mus_test, ys_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142333d-3ad3-4c0e-8b4a-d1b4e3a99d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dci_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ec6d4-628c-477c-84da-acb263d3a3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SAP\n",
    "from sklearn import svm\n",
    "\n",
    "def compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes the SAP score.\n",
    "\n",
    "    Args:\n",
    "        mus, ys, mus_test, ys_test\n",
    "        continuous_factors: Factors are continuous variable (True) or not (False).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with SAP score.\n",
    "    \"\"\"\n",
    "\n",
    "    return _compute_sap(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "\n",
    "def _compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "    score_matrix = compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "    # Score matrix should have shape [num_latents, num_factors].\n",
    "    assert score_matrix.shape[0] == mus.shape[0]\n",
    "    assert score_matrix.shape[1] == ys.shape[0]\n",
    "    scores_dict = {}\n",
    "    scores_dict[\"SAP_score\"] = compute_avg_diff_top_two(score_matrix)\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "def compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Compute score matrix as described in Section 3.\"\"\"\n",
    "    num_latents = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    score_matrix = np.zeros([num_latents, num_factors])\n",
    "    for i in range(num_latents):\n",
    "        for j in range(num_factors):\n",
    "            mu_i = mus[i, :]\n",
    "            y_j = ys[j, :]\n",
    "            if continuous_factors:\n",
    "                # Attribute is considered continuous.\n",
    "                cov_mu_i_y_j = np.cov(mu_i, y_j, ddof=1)\n",
    "                cov_mu_y = cov_mu_i_y_j[0, 1]**2\n",
    "                var_mu = cov_mu_i_y_j[0, 0]\n",
    "                var_y = cov_mu_i_y_j[1, 1]\n",
    "                if var_mu > 1e-12:\n",
    "                    score_matrix[i, j] = cov_mu_y * 1. / (var_mu * var_y)\n",
    "                else:\n",
    "                    score_matrix[i, j] = 0.\n",
    "            else:\n",
    "                # Attribute is considered discrete.\n",
    "                mu_i_test = mus_test[i, :]\n",
    "                y_j_test = ys_test[j, :]\n",
    "                classifier = svm.LinearSVC(C=0.01, class_weight=\"balanced\")\n",
    "                classifier.fit(mu_i[:, np.newaxis], y_j)\n",
    "                pred = classifier.predict(mu_i_test[:, np.newaxis])\n",
    "                score_matrix[i, j] = np.mean(pred == y_j_test)\n",
    "    return score_matrix\n",
    "\n",
    "def compute_avg_diff_top_two(matrix):\n",
    "    sorted_matrix = np.sort(matrix, axis=0)\n",
    "    return np.mean(sorted_matrix[-1, :] - sorted_matrix[-2, :])\n",
    "\n",
    "\n",
    "sap = compute_sap(mus_train, ys_train, mus_test, ys_test, continuous_factors=False)\n",
    "sap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1d0bc-6e03-4ca2-ad6e-20e5ba8e52f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IRS \n",
    "\n",
    "\n",
    "def compute_irs(mus, ys, diff_quantile=0.99):\n",
    "    \"\"\"Computes the Interventional Robustness Score.\n",
    "\n",
    "    Args:\n",
    "        ground_truth_data: GroundTruthData to be sampled from.\n",
    "        representation_function: Function that takes observations as input and\n",
    "            outputs a dim_representation sized representation for each observation.\n",
    "        random_state: Numpy random state used for randomness.\n",
    "        artifact_dir: Optional path to directory where artifacts can be saved.\n",
    "        diff_quantile: Float value between 0 and 1 to decide what quantile of diffs\n",
    "            to select (use 1.0 for the version in the paper).\n",
    "        num_train: Number of points used for training.\n",
    "        batch_size: Batch size for sampling.\n",
    "\n",
    "    Returns:\n",
    "        Dict with IRS and number of active dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    ys_discrete = make_discretizer(ys)\n",
    "    active_mus = _drop_constant_dims(mus)\n",
    "\n",
    "    if not active_mus.any():\n",
    "        irs_score = 0.0\n",
    "    else:\n",
    "        irs_score = scalable_disentanglement_score(ys_discrete.T, active_mus.T,  diff_quantile)[\"avg_score\"]\n",
    "\n",
    "    score_dict = {}\n",
    "    score_dict[\"IRS\"] = irs_score\n",
    "    score_dict[\"num_active_dims\"] = active_mus.shape[0]\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def _drop_constant_dims(ys):\n",
    "    \"\"\"Returns a view of the matrix `ys` with dropped constant rows.\"\"\"\n",
    "    ys = np.asarray(ys)\n",
    "    if ys.ndim != 2:\n",
    "        raise ValueError(\"Expecting a matrix.\")\n",
    "\n",
    "    variances = ys.var(axis=1)\n",
    "    active_mask = variances > 0.\n",
    "    return ys[active_mask, :]\n",
    "\n",
    "\n",
    "def scalable_disentanglement_score(gen_factors, latents, diff_quantile=0.99):\n",
    "    \"\"\"Computes IRS scores of a dataset.\n",
    "\n",
    "    Assumes no noise in X and crossed generative factors (i.e. one sample per\n",
    "    combination of gen_factors). Assumes each g_i is an equally probable\n",
    "    realization of g_i and all g_i are independent.\n",
    "\n",
    "    Args:\n",
    "        gen_factors: Numpy array of shape (num samples, num generative factors),\n",
    "            matrix of ground truth generative factors.\n",
    "        latents: Numpy array of shape (num samples, num latent dimensions), matrix\n",
    "            of latent variables.\n",
    "        diff_quantile: Float value between 0 and 1 to decide what quantile of diffs\n",
    "            to select (use 1.0 for the version in the paper).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with IRS scores.\n",
    "    \"\"\"\n",
    "    num_gen = gen_factors.shape[1]\n",
    "    num_lat = latents.shape[1]\n",
    "\n",
    "    # Compute normalizer.\n",
    "    max_deviations = np.max(np.abs(latents - latents.mean(axis=0)), axis=0)\n",
    "    cum_deviations = np.zeros([num_lat, num_gen])\n",
    "    for i in range(num_gen):\n",
    "        unique_factors = np.unique(gen_factors[:, i], axis=0)\n",
    "        assert unique_factors.ndim == 1\n",
    "        num_distinct_factors = unique_factors.shape[0]\n",
    "        for k in range(num_distinct_factors):\n",
    "            # Compute E[Z | g_i].\n",
    "            match = gen_factors[:, i] == unique_factors[k]\n",
    "            e_loc = np.mean(latents[match, :], axis=0)\n",
    "\n",
    "            # Difference of each value within that group of constant g_i to its mean.\n",
    "            diffs = np.abs(latents[match, :] - e_loc)\n",
    "            max_diffs = np.percentile(diffs, q=diff_quantile*100, axis=0)\n",
    "            cum_deviations[:, i] += max_diffs\n",
    "        cum_deviations[:, i] /= num_distinct_factors\n",
    "    # Normalize value of each latent dimension with its maximal deviation.\n",
    "    normalized_deviations = cum_deviations / max_deviations[:, np.newaxis]\n",
    "    irs_matrix = 1.0 - normalized_deviations\n",
    "    disentanglement_scores = irs_matrix.max(axis=1)\n",
    "    if np.sum(max_deviations) > 0.0:\n",
    "        avg_score = np.average(disentanglement_scores, weights=max_deviations)\n",
    "    else:\n",
    "        avg_score = np.mean(disentanglement_scores)\n",
    "\n",
    "    parents = irs_matrix.argmax(axis=1)\n",
    "    score_dict = {}\n",
    "    score_dict[\"disentanglement_scores\"] = disentanglement_scores\n",
    "    score_dict[\"avg_score\"] = avg_score\n",
    "    score_dict[\"parents\"] = parents\n",
    "    score_dict[\"IRS_matrix\"] = irs_matrix\n",
    "    score_dict[\"max_deviations\"] = max_deviations\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "\n",
    "irs = compute_irs(mus_train, ys_train, diff_quantile=0.99)\n",
    "irs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fba79-0ac1-45c3-ac97-69de1147e5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
