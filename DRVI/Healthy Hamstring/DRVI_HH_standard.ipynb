{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ce1ce-8ea3-4389-9c99-b8fb91b6e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac983c6b-f1da-458a-af96-32eff906d2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/theislab/drvi.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be987206-7915-46e9-a685-19175875a026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install gprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be2af9-9421-4041-9ed1-6f76c9ba6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b08b7b-a395-4297-9f1b-e6fdc326be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598c65d-8b65-4671-9208-32c4dd7dbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f74929-8523-4af6-872a-fcc5a5c04199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139393e9-3279-4343-a6d0-94a7a20903a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from gprofiler import gprofiler\n",
    "import torch\n",
    "import numpy as np\n",
    "import drvi\n",
    "from drvi.model import DRVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a19ac9-a125-4499-a172-48a144faf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(dpi=100, frameon=False)\n",
    "sc.set_figure_params(dpi=100)\n",
    "sc.set_figure_params(figsize=(3, 3))\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8bd45-7e7b-46e9-9351-a72a6fc897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"/work/DRVI/fixed_data/HH/healthy_hamstring_processed_adata_raw_nonormalization.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98ee41-1507-4be3-8289-f025d6f241f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e8d8f-7512-4b23-9641-20999c582854",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a944b3-77d1-4182-8517-55ec180b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "split_key = \"split\"\n",
    "adata.obs[split_key] = \"train\"\n",
    "idx = list(range(len(adata)))\n",
    "idx_train, idx_test = train_test_split(adata.obs_names, test_size=0.1, random_state=42)\n",
    "adata.obs.loc[idx_train, split_key] = \"train\"\n",
    "adata.obs.loc[idx_test, split_key] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1c151-b5ec-40e5-b2ec-71207570bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adata = adata[adata.obs[split_key] == \"train\"].copy()\n",
    "test_adata = adata[adata.obs[split_key] == \"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1271916-d92e-41a4-8a07-35ba2526c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adata.write(\"test_adata_CHECKING.h5ad\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63ec8cc1-d260-4cec-b98c-91967d7137ce",
   "metadata": {},
   "source": [
    "train_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec24221-4aec-4343-beec-c0b25fc5a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "DRVI.setup_anndata(\n",
    "    train_adata,\n",
    "    # DRVI accepts count data by default.\n",
    "    # Do not forget to change gene_likelihood if you provide a non-count data.\n",
    "    layer=\"counts\",\n",
    "    # Always provide a list. DRVI can accept multiple covariates.\n",
    "    categorical_covariate_keys=[\"cell_type\", \"sex\", \"donor_id\"],\n",
    "    # DRVI accepts count data by default.\n",
    "    # Set to false if you provide log-normalized data and use normal distribution (mse loss).\n",
    "    is_count_data=True,\n",
    ")\n",
    "\n",
    "# construct the model\n",
    "model = DRVI(\n",
    "    train_adata,\n",
    "    # Provide categorical covariates keys once again. Refer to advanced usages for more options.\n",
    "    categorical_covariates=[\"cell_type\", \"sex\", \"donor_id\"],\n",
    "    n_latent=128,\n",
    "    # For encoder and decoder dims, provide a list of integers.\n",
    "    encoder_dims=[128, 128],\n",
    "    decoder_dims=[128, 128],\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea82b65-0ee0-4680-88f0-dbf7435672bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cpu training you should add the following line to the model.train parameters:\n",
    "#accelerator=\"cpu\", devices=1,\n",
    "#\n",
    "# For mps acceleration on macbooks, add the following line to the model.train parameters:\n",
    "# accelerator=\"mps\", devices=1,\n",
    "#\n",
    "# For gpu training don't provide any additional parameter.\n",
    "# More details here: https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html\n",
    "\n",
    "n_epochs = 400\n",
    "\n",
    "# train the model\n",
    "model.train(\n",
    "    max_epochs=n_epochs,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=20,\n",
    "    # mps\n",
    "    # accelerator=\"mps\", devices=1,\n",
    "    # cpu\n",
    "    #accelerator=\"cpu\", devices=1,\n",
    "    # gpu: no additional parameter\n",
    "    #\n",
    "    # No need to provide `plan_kwargs` if n_epochs >= 400.\n",
    "    plan_kwargs={\n",
    "        \"n_epochs_kl_warmup\": n_epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Runtime:\n",
    "# The runtime for CPU laptop (M1) is 208 minutes\n",
    "# The runtime for Macbook gpu (M1) is 64 minutes\n",
    "# The runtime for GPU (A100) is 17 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08065aa8-aef6-4074-a341-16ae26907fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"FIXED_trained_models/DRVI_HH_final_covariate_check/\", adata = adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0415ea5-fe54-44ec-8f31-18ed2dd51c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"/work/DRVI/FIXED_trained_models/DRVI_HH_final_raw\", adata = train_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16daee-31e4-4827-a090-988e0efe28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19288695-b397-4173-911f-18f8f29867c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, adata):\n",
    "    model._validate_anndata(adata)\n",
    "    model.module.eval()\n",
    "\n",
    "    scdl = model._make_data_loader(adata=adata, indices=None, batch_size=128, shuffle=False)\n",
    "    mus = []\n",
    "    for tensors in scdl:\n",
    "        inference_outputs, generative_outputs = model.module.forward(\n",
    "                    tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "        _mus = torch.nan_to_num(generative_outputs['px'].mean, nan=0, neginf=0, posinf=100) \n",
    "        mus.append(_mus.detach().cpu().numpy())\n",
    "    mus = np.concatenate(mus, axis=0)\n",
    "    out_adata = adata.copy()\n",
    "    out_adata.X = mus\n",
    "    return out_adata\n",
    "\n",
    "\n",
    "model._validate_anndata(test_adata)\n",
    "rec = predict(model, test_adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b53990-e973-4c11-8f75-b2dd0b0519d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_post.write(\"adata_post_with_reconstruction_DRVI_fixed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d17242-6b1f-4861-9966-9b54127f908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If rec is an AnnData object, extract the X attribute (i.e., the data matrix)\n",
    "import anndata\n",
    "if isinstance(rec, anndata.AnnData):\n",
    "    rec = rec.X\n",
    "\n",
    "# Now, rec should be a numpy array or sparse matrix, which is what obsm expects\n",
    "test_adata.obsm[\"X_reconstructed\"] = rec\n",
    "\n",
    "# Save the entire object with the reconstructed data\n",
    "test_adata.write(\"adata_post_with_latent_and_reconstructed_HH_DRVI_raw_fix.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645d170-e676-47be-b616-e82d08f3e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc667d0-5aa0-46a9-bc5a-b3fa0d83d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = rec.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4c8d5-a90b-4d69-b584-ec3dde94e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210ea8e-0230-4f1a-9a16-299d71c7d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "# Ensure y_true is dense\n",
    "y_true = test_adata.X\n",
    "if not isinstance(y_true, np.ndarray):\n",
    "    y_true = y_true.toarray()\n",
    "\n",
    "# Ensure rec is dense\n",
    "if isinstance(rec, anndata.AnnData):  # Only access .X if rec is an AnnData object\n",
    "    rec = rec.X\n",
    "\n",
    "if not isinstance(rec, np.ndarray):   # Convert to dense if it's still sparse\n",
    "    rec = rec.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b481a-4c91-45e6-9a7f-a69cd7b3d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten for R²\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = rec1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f55fa-6b80-459a-93c0-9d25a758321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = model.get_latent_representation(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714f1a3-66bc-461b-bec5-04402888b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R²\n",
    "r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b616c-6a98-4c7c-9919-c53e18134890",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b5f85-8861-4ac3-9a7b-adf6ea23c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435d816-0988-4c60-8865-c87c384dcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.read(\"adata_post_with_reconstruction_DRVI.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76650c0-2647-4c46-85ba-9801c8e8b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_post = test_adata.copy()\n",
    "adata_post.obsm[\"X_reconstructed_DRVI\"] = rec.X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477dedac-8482-41f7-9f3e-bc40438135ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))  # Replace `model` with your DRVI model variab'\n",
    "model.get_latent_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c796d7-d5e5-4efa-bc85-9ed8a01271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.get_latent_representation(\n",
    "    adata,\n",
    "    batch_size=256,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f0b4a-53d1-4731-95ca-a7a7c82e95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "latent_adata = sc.AnnData(z)\n",
    "latent_adata.obs = train_adata.obs.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14f2ab-b9b2-4271-8750-21f315197f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47998c7-80bf-4f56-9899-8ae3661ca302",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(latent_adata, use_rep=\"X\")  # Use latent space for neighbor graph\n",
    "sc.tl.umap(latent_adata)  # Run UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada23119-59f4-4dec-b445-b97e448f7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(latent_adata, color=[\"cell_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec102f6-147e-4a55-93fd-f9114103cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(z, n_neighbors=10, use_rep=\"X\", n_pcs=embed.X.shape[1])\n",
    "sc.tl.umap(z, spread=1.0, min_dist=0.5, random_state=123)\n",
    "sc.pp.pca(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df3d9e-3271-4388-bcd1-55f2a7f30ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(z, color=[\"cell_type\"], ncols=1, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f3ff9-8f10-423e-be30-84f0a551ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_basal = model.get_latent_representation(\n",
    "    adata,\n",
    "    batch_size=256,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184a0cc-efb3-4b94-8a18-e45722d27621",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = ad.AnnData(model.get_latent_representation(), obs=train_adata.obs)\n",
    "sc.pp.subsample(embed, fraction=1.0)  # Shuffling for better visualization of overlapping colors\n",
    "\n",
    "sc.pp.neighbors(embed, n_neighbors=10, use_rep=\"X\", n_pcs=embed.X.shape[1])\n",
    "sc.tl.umap(embed, spread=1.0, min_dist=0.5, random_state=123)\n",
    "sc.pp.pca(embed)\n",
    "\n",
    "embed.write(\"drvi_HH_128_embed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80756a5-410a-4551-b85f-41077d85041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = sc.read(\"drvi_HH_128_embed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5594e5f-f451-4587-b5b9-e385bde0dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(embed, color=[\"cell_type\"], ncols=1, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630458a-1b96-4da9-8dce-9bdedb954ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47caba2e-aa2e-4fb0-a0c5-832d94077f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62ead3-8000-4a76-bff6-8e15df34562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb61dc8-e5b7-4074-a23d-a40e954aeee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of y_true (filtered adata): {adata_2.shape}\")  # Should be (1002, 22692)\n",
    "print(f\"Shape of y_pred (rec): {rec_2.shape}\")  # Should be (1002, 22692)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac158fe-632a-409c-bad5-a2aff63f048f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "###MIG###\n",
    "import numpy as np\n",
    "import anndata\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical(data):\n",
    "    encoders = []\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "        encoders.append(le)\n",
    "    return encoded_data, encoders\n",
    "\n",
    "def prep_data(adata, embedding, covriate_keys=None):\n",
    "    encoded_factors_of_variation, _ = encode_categorical(adata.obs[covriate_keys].values)\n",
    "\n",
    "    if isinstance(embedding, anndata.AnnData):  \n",
    "        embedding_data = embedding.X\n",
    "    else:\n",
    "        embedding_data = embedding\n",
    "\n",
    "    mus = np.array(embedding_data)\n",
    "    ys = np.array(encoded_factors_of_variation)\n",
    "\n",
    "    return mus.T.copy(), ys.T.copy()\n",
    "\n",
    "\n",
    "def compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes the mutual information gap.\"\"\"\n",
    "    return _compute_mig(mus, ys, covariate_names)\n",
    "\n",
    "def _compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes MIG score based on latent codes and covariates.\"\"\"\n",
    "    score_dict = {}\n",
    "    discretized_mus = make_discretizer(mus, discretizer_fn=_histogram_discretize)\n",
    "   # print(\"Sample Discretized Latent Variables:\\n\", discretized_mus[:, :5])\n",
    "    m = discrete_mutual_info(discretized_mus, ys)\n",
    "\n",
    "    if covariate_names is None:\n",
    "        covariate_names = [f\"Covariate {j}\" for j in range(m.shape[1])]\n",
    "        \n",
    "    for j in range(m.shape[1]):\n",
    "        top_indices = np.argsort(m[:, j])[::-1][:3]\n",
    "        top_scores = m[top_indices, j]\n",
    "        print(f\"Top 3 MI scores for covariate '{covariate_names[j]}':\")\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            print(f\"  Latent dim {idx}: MI = {score:.4f}\")\n",
    "\n",
    "    assert m.shape[0] == mus.shape[0]\n",
    "    assert m.shape[1] == ys.shape[0]\n",
    "\n",
    "    entropy = discrete_entropy(ys)\n",
    "    sorted_m = np.sort(m, axis=0)[::-1]\n",
    "\n",
    "    score_dict[\"discrete_mig\"] = np.mean(\n",
    "        np.divide(sorted_m[0, :] - sorted_m[1, :], entropy[:])\n",
    "    )\n",
    "\n",
    "    print(\"MIG score:\", score_dict)\n",
    "    print(\"Entropy values:\", entropy)\n",
    "    return score_dict\n",
    "\n",
    "def discrete_mutual_info(mus, ys):\n",
    "    num_codes = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    m = np.zeros([num_codes, num_factors])\n",
    "    \n",
    "    for i in range(num_codes):\n",
    "        for j in range(num_factors):\n",
    "            m[i, j] = mutual_info_score(ys[j, :], mus[i, :])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def discrete_entropy(ys):\n",
    "    num_factors = ys.shape[0]\n",
    "    h = np.zeros(num_factors)\n",
    "    \n",
    "    for j in range(num_factors):\n",
    "        h[j] = mutual_info_score(ys[j, :], ys[j, :])\n",
    "    \n",
    "    return h\n",
    "\n",
    "def _identity_discretizer(target, num_bins):\n",
    "    del num_bins\n",
    "    return target\n",
    "\n",
    "\n",
    "def _histogram_discretize(target, num_bins=10):\n",
    "    discretized = np.zeros_like(target)\n",
    "    for i in range(target.shape[0]):\n",
    "        discretized[i, :] = np.digitize(target[i, :], np.histogram(\n",
    "            target[i, :], num_bins)[1][:-1])\n",
    "    return discretized\n",
    "\n",
    "\n",
    "def make_discretizer(target, num_bins=10, discretizer_fn=_histogram_discretize):\n",
    "    return discretizer_fn(target, num_bins)\n",
    "\n",
    "\n",
    "def score_disentanglement(adata, embedding_data, embedding_basal, covriate_keys=None, continuous_covriate_keys=None):\n",
    "    mus, ys = prep_data(adata, embedding_data, covriate_keys=covriate_keys)\n",
    "    print('Computing MIG')\n",
    "    mig = compute_mig(mus, ys, covariate_names=covriate_keys)\n",
    "    return mig, mus, ys\n",
    "\n",
    "# Run MIG score\n",
    "mig_1,mus,ys = score_disentanglement(\n",
    "    adata,\n",
    "    latent,\n",
    "    None,\n",
    "    covriate_keys=[\"cell_type\", \"sex\", \"donor_id\"]\n",
    ")\n",
    "\n",
    "print(\"MIG Score:\", mig_1)\n",
    "print(\"Latent variances:\", np.var(mus, axis=1))\n",
    "\n",
    "discretized_mus = _histogram_discretize(mus)\n",
    "m = discrete_mutual_info(discretized_mus, ys)\n",
    "\n",
    "print(\"MI matrix shape:\", m.shape)\n",
    "print(\"Max MI per factor:\", np.max(m, axis=0))\n",
    "print(\"Which latents have highest MI per factor:\", np.argmax(m, axis=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc95d8a-3d3f-49df-951d-36cee8f9b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized DCI computation based on disentanglement_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Encoding and Preprocessing ===\n",
    "def encode_categorical(data):\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "    return encoded_data\n",
    "\n",
    "def remove_duplicate_columns(df):\n",
    "    df_unique = df.T.drop_duplicates().T\n",
    "    return df_unique\n",
    "\n",
    "def prep_data(adata, embedding, covariate_keys, test_size=0.25):\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        range(len(adata)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    cov_df = adata.obs[covariate_keys].copy()\n",
    "    cov_df = remove_duplicate_columns(cov_df)\n",
    "    encoded_factors = encode_categorical(cov_df.values)\n",
    "    embedding_data = embedding.X if isinstance(embedding, anndata.AnnData) else embedding\n",
    "    mus_train = embedding_data[idx_train]\n",
    "    mus_test = embedding_data[idx_test]\n",
    "    ys_train = encoded_factors[idx_train]\n",
    "    ys_test = encoded_factors[idx_test]\n",
    "    return mus_train.T, ys_train.T, mus_test.T, ys_test.T\n",
    "\n",
    "# === Importance Matrix ===\n",
    "def compute_importance_rf(x_train, y_train, x_test, y_test):\n",
    "    num_factors = y_train.shape[0]\n",
    "    num_codes = x_train.shape[0]\n",
    "    importance_matrix = np.zeros((num_codes, num_factors))\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(num_factors):\n",
    "        model = RandomForestClassifier(random_state=42, max_depth=5)\n",
    "        model.fit(x_train.T, y_train[i])\n",
    "        importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "        train_acc.append(np.mean(model.predict(x_train.T) == y_train[i]))\n",
    "        test_acc.append(np.mean(model.predict(x_test.T) == y_test[i]))\n",
    "    return importance_matrix, np.mean(train_acc), np.mean(test_acc)\n",
    "\n",
    "# === Disentanglement ===\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "    row_sums = importance_matrix.sum(axis=1, keepdims=True)\n",
    "    safe_matrix = np.where(row_sums == 0, 1e-11, row_sums)\n",
    "    normalized = importance_matrix / safe_matrix\n",
    "    return 1. - entropy(normalized.T + 1e-11, base=importance_matrix.shape[1])\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "    per_code = disentanglement_per_code(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    code_importance = importance_matrix.sum(axis=1) / total\n",
    "    return np.sum(per_code * code_importance)\n",
    "\n",
    "# === Completeness ===\n",
    "def completeness_per_factor(importance_matrix):\n",
    "    return 1. - entropy(importance_matrix + 1e-11, base=importance_matrix.shape[0])\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "    per_factor = completeness_per_factor(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    factor_importance = importance_matrix.sum(axis=0) / total\n",
    "    return np.sum(per_factor * factor_importance)\n",
    "\n",
    "# === DCI Master Function ===\n",
    "def compute_dci(mus_train, ys_train, mus_test, ys_test):\n",
    "    importance_matrix, train_acc, test_acc = compute_importance_rf(\n",
    "        mus_train, ys_train, mus_test, ys_test\n",
    "    )\n",
    "    threshold = 1e-11\n",
    "    importance_matrix = np.where(importance_matrix < threshold, 0, importance_matrix)\n",
    "    return {\n",
    "    \"disentanglement\": disentanglement(importance_matrix),\n",
    "    \"completeness\": completeness(importance_matrix),\n",
    "    \"importance_matrix\": importance_matrix,\n",
    "    \"train_acc\": train_acc,\n",
    "    \"test_acc\": test_acc,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babcb2e-c6cd-4f47-9a9e-4ee2644f3eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "covariate_keys = [\"cell_type\", \"sex\", \"donor_id\"]\n",
    "mus_train, ys_train, mus_test, ys_test = prep_data(adata, latent, covariate_keys)\n",
    "dci_scores = compute_dci(mus_train, ys_train, mus_test, ys_test)\n",
    "\n",
    "\n",
    "print(\"Disentanglement Score:\", dci_scores[\"disentanglement\"])\n",
    "print(\"Completeness Score:\", dci_scores[\"completeness\"])\n",
    "print(\"Train Accuracy:\", dci_scores[\"train_acc\"])\n",
    "print(\"Test Accuracy:\", dci_scores[\"test_acc\"])\n",
    "\n",
    "# Access importance matrix if needed\n",
    "importance_matrix = dci_scores[\"importance_matrix\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097bf73-244f-4cf7-af9c-6fbadaedef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAP score\n",
    "from sklearn import svm\n",
    "\n",
    "def compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes the SAP score.\n",
    "\n",
    "    Args:\n",
    "        mus, ys, mus_test, ys_test\n",
    "        continuous_factors: Factors are continuous variable (True) or not (False).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with SAP score.\n",
    "    \"\"\"\n",
    "\n",
    "    return _compute_sap(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "\n",
    "def _compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "    score_matrix = compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "    # Score matrix should have shape [num_latents, num_factors].\n",
    "    assert score_matrix.shape[0] == mus.shape[0]\n",
    "    assert score_matrix.shape[1] == ys.shape[0]\n",
    "    scores_dict = {}\n",
    "    scores_dict[\"SAP_score\"] = compute_avg_diff_top_two(score_matrix)\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "def compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Compute score matrix as described in Section 3.\"\"\"\n",
    "    num_latents = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    score_matrix = np.zeros([num_latents, num_factors])\n",
    "    for i in range(num_latents):\n",
    "        for j in range(num_factors):\n",
    "            mu_i = mus[i, :]\n",
    "            y_j = ys[j, :]\n",
    "            if continuous_factors:\n",
    "                # Attribute is considered continuous.\n",
    "                cov_mu_i_y_j = np.cov(mu_i, y_j, ddof=1)\n",
    "                cov_mu_y = cov_mu_i_y_j[0, 1]**2\n",
    "                var_mu = cov_mu_i_y_j[0, 0]\n",
    "                var_y = cov_mu_i_y_j[1, 1]\n",
    "                if var_mu > 1e-12:\n",
    "                    score_matrix[i, j] = cov_mu_y * 1. / (var_mu * var_y)\n",
    "                else:\n",
    "                    score_matrix[i, j] = 0.\n",
    "            else:\n",
    "                # Attribute is considered discrete.\n",
    "                mu_i_test = mus_test[i, :]\n",
    "                y_j_test = ys_test[j, :]\n",
    "                classifier = svm.LinearSVC(C=0.01, class_weight=\"balanced\")\n",
    "                classifier.fit(mu_i[:, np.newaxis], y_j)\n",
    "                pred = classifier.predict(mu_i_test[:, np.newaxis])\n",
    "                score_matrix[i, j] = np.mean(pred == y_j_test)\n",
    "    return score_matrix\n",
    "\n",
    "def compute_avg_diff_top_two(matrix):\n",
    "    sorted_matrix = np.sort(matrix, axis=0)\n",
    "    return np.mean(sorted_matrix[-1, :] - sorted_matrix[-2, :])\n",
    "\n",
    "sap = compute_sap(mus_train, ys_train, mus_test, ys_test, continuous_factors=False)\n",
    "sap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a28a87-d841-438e-8856-2a398e9d7bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IRS \n",
    "\n",
    "\n",
    "def compute_irs(mus, ys, diff_quantile=0.99):\n",
    "    ys_discrete = make_discretizer(ys)\n",
    "\n",
    "    active_mask = (mus.var(axis=1) > 0)\n",
    "    active_mus = mus[active_mask, :]\n",
    "\n",
    "    if active_mus.size == 0:\n",
    "        irs_score = 0.0\n",
    "    else:\n",
    "        irs_score = scalable_disentanglement_score(ys_discrete.T, active_mus.T, diff_quantile)[\"avg_score\"]\n",
    "\n",
    "    score_dict = {}\n",
    "    score_dict[\"IRS\"] = irs_score\n",
    "    score_dict[\"num_active_dims\"] = int(np.sum(active_mask))\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def _drop_constant_dims(ys):\n",
    "    \"\"\"Returns a view of the matrix `ys` with dropped constant rows.\"\"\"\n",
    "    ys = np.asarray(ys)\n",
    "    if ys.ndim != 2:\n",
    "        raise ValueError(\"Expecting a matrix.\")\n",
    "\n",
    "    variances = ys.var(axis=1)\n",
    "    active_mask = variances > 0.\n",
    "    return ys[active_mask, :]\n",
    "\n",
    "\n",
    "def scalable_disentanglement_score(gen_factors, latents, diff_quantile=0.99):\n",
    "    \"\"\"Computes IRS scores of a dataset.\n",
    "\n",
    "    Assumes no noise in X and crossed generative factors (i.e. one sample per\n",
    "    combination of gen_factors). Assumes each g_i is an equally probable\n",
    "    realization of g_i and all g_i are independent.\n",
    "\n",
    "    Args:\n",
    "        gen_factors: Numpy array of shape (num samples, num generative factors),\n",
    "            matrix of ground truth generative factors.\n",
    "        latents: Numpy array of shape (num samples, num latent dimensions), matrix\n",
    "            of latent variables.\n",
    "        diff_quantile: Float value between 0 and 1 to decide what quantile of diffs\n",
    "            to select (use 1.0 for the version in the paper).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with IRS scores.\n",
    "    \"\"\"\n",
    "    num_gen = gen_factors.shape[1]\n",
    "    num_lat = latents.shape[1]\n",
    "\n",
    "    # Compute normalizer.\n",
    "    max_deviations = np.max(np.abs(latents - latents.mean(axis=0)), axis=0)\n",
    "    cum_deviations = np.zeros([num_lat, num_gen])\n",
    "    for i in range(num_gen):\n",
    "        unique_factors = np.unique(gen_factors[:, i], axis=0)\n",
    "        assert unique_factors.ndim == 1\n",
    "        num_distinct_factors = unique_factors.shape[0]\n",
    "        for k in range(num_distinct_factors):\n",
    "            # Compute E[Z | g_i].\n",
    "            match = gen_factors[:, i] == unique_factors[k]\n",
    "            e_loc = np.mean(latents[match, :], axis=0)\n",
    "\n",
    "            # Difference of each value within that group of constant g_i to its mean.\n",
    "            diffs = np.abs(latents[match, :] - e_loc)\n",
    "            max_diffs = np.percentile(diffs, q=diff_quantile*100, axis=0)\n",
    "            cum_deviations[:, i] += max_diffs\n",
    "        cum_deviations[:, i] /= num_distinct_factors\n",
    "    # Normalize value of each latent dimension with its maximal deviation.\n",
    "    normalized_deviations = cum_deviations / max_deviations[:, np.newaxis]\n",
    "    irs_matrix = 1.0 - normalized_deviations\n",
    "    disentanglement_scores = irs_matrix.max(axis=1)\n",
    "    if np.sum(max_deviations) > 0.0:\n",
    "        avg_score = np.average(disentanglement_scores, weights=max_deviations)\n",
    "    else:\n",
    "        avg_score = np.mean(disentanglement_scores)\n",
    "\n",
    "    parents = irs_matrix.argmax(axis=1)\n",
    "    score_dict = {}\n",
    "    score_dict[\"disentanglement_scores\"] = disentanglement_scores\n",
    "    score_dict[\"avg_score\"] = avg_score\n",
    "    score_dict[\"parents\"] = parents\n",
    "    score_dict[\"IRS_matrix\"] = irs_matrix\n",
    "    score_dict[\"max_deviations\"] = max_deviations\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "\n",
    "irs = compute_irs(mus, ys, diff_quantile=1)\n",
    "print(irs)\n",
    "\n",
    "\n",
    "print(\"mus shape:\", mus.shape)\n",
    "\n",
    "\n",
    "def compute_latent_sensitivity(mus, ys):\n",
    "    num_latents = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    sensitivity = np.zeros((num_latents, num_factors))\n",
    "\n",
    "    for i in range(num_factors):  # for each generative factor\n",
    "        factor_vals = np.unique(ys[i])\n",
    "        group_means = []\n",
    "\n",
    "        for val in factor_vals:\n",
    "            idx = ys[i] == val\n",
    "            mean_latents = np.mean(mus[:, idx], axis=1)  # shape: (num_latents,)\n",
    "            group_means.append(mean_latents)\n",
    "\n",
    "        group_means = np.stack(group_means, axis=1)  # shape: (num_latents, num_values)\n",
    "        sensitivity[:, i] = np.std(group_means, axis=1)\n",
    "\n",
    "    return sensitivity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Remove suspicious latents\n",
    "mus_cleaned = np.delete(mus, suspicious_latents, axis=0)\n",
    "\n",
    "irs_cleaned = compute_irs(mus_cleaned, ys, diff_quantile=0.99)\n",
    "print(\"IRS after removing suspicious latents:\", irs_cleaned)\n",
    "\n",
    "sensitivity_matrix = compute_latent_sensitivity(mus, ys)  # shape: (latent_dim, factor_dim)\n",
    "#print(sensitivity_matrix)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latent_dim, factor_dim = sensitivity_matrix.shape\n",
    "for i in range(factor_dim):\n",
    "    plt.scatter(sensitivity_matrix[:, i], irs_matrix[:, i])\n",
    "    plt.xlabel(f\"Sensitivity to factor {i}\")\n",
    "    plt.ylabel(f\"IRS for factor {i}\")\n",
    "    plt.title(f\"IRS vs Sensitivity (Factor {i})\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def compute_mutual_info_matrix(mus, ys):\n",
    "    num_latents = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    mi_matrix = np.zeros((num_latents, num_factors))\n",
    "\n",
    "    for j in range(num_latents):\n",
    "        for i in range(num_factors):\n",
    "            mi_matrix[j, i] = mutual_info_regression(ys[i:i+1].T, mus[j], discrete_features=True)[0]\n",
    "\n",
    "    return mi_matrix\n",
    "mi_matrix = compute_mutual_info_matrix(mus, ys)  # shape: (latent_dim, factor_dim)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.heatmap(mi_matrix, cmap='viridis')\n",
    "#plt.title(\"Mutual Information between Latents and Factors\")\n",
    "#plt.xlabel(\"Factor Index\")\n",
    "#plt.ylabel(\"Latent Dimension\")\n",
    "#plt.show()\n",
    "\n",
    "suspicious_latents = []\n",
    "for j in range(mus.shape[0]):\n",
    "    if np.all(sensitivity_matrix[j] < 0.01):  # low variation\n",
    "        if np.sum(mi_matrix[j] > 0.01) > 1:    # entangled\n",
    "            suspicious_latents.append(j)\n",
    "print(\"Suspicious latents (stable + entangled):\", suspicious_latents)\n",
    "\n",
    "\n",
    "max_devs = irs_full[\"max_deviations\"]\n",
    "disent_scores = irs_full[\"disentanglement_scores\"]\n",
    "\n",
    "plt.scatter(max_devs, disent_scores)\n",
    "plt.xlabel(\"Max Deviation (overall activity)\")\n",
    "plt.ylabel(\"IRS Score\")\n",
    "plt.title(\"Latent Activity vs IRS\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
