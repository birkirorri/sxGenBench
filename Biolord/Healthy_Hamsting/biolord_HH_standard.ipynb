{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b25689-f80c-4c48-88b1-b20b1ee947ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install biolord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb404ef-f1e5-4bf0-a559-3443bad3f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import biolord\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mutual_info_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57574ea6-9b78-448e-a833-a3c381fa3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"/work/Biolord_all/new_data_raw/healthy_hamstring_processed_adata_raw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a4cd0-c744-4470-81f6-230e31b1c96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653806f3-1cd7-4ac0-b26b-3753898ec2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da19eb-e9a6-42da-8ce8-33fd301f7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "split_key = \"split\"\n",
    "adata.obs[split_key] = \"train\"\n",
    "idx = list(range(len(adata)))\n",
    "idx_train, idx_test = train_test_split(adata.obs_names, test_size=0.1, random_state=42)\n",
    "adata.obs.loc[idx_train, split_key] = \"train\"\n",
    "adata.obs.loc[idx_test, split_key] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f21f9e-0e39-4f53-baf7-0e386cb860b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_normal = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026daf54-5782-4771-b045-9770928d4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d4703-ebd7-4828-b1c8-cdb2ad9b0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing to median total counts\n",
    "sc.pp.normalize_total(adata_normal, target_sum=1e4)\n",
    "# Logarithmize the data\n",
    "sc.pp.log1p(adata_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2bd49-4e3a-47a5-8a21-51e9b64952b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ac89e-c0dc-452d-89a7-7e10691da301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to use only the training set and make a copy\n",
    "adata_train = adata_normal[adata_normal.obs[split_key] == \"train\"].copy()\n",
    "adata_test = adata_normal[adata_normal.obs[split_key] == \"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c663dfe-2691-4a32-a98e-6bfb4a3e37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "biolord.Biolord.setup_anndata(\n",
    "    adata_normal, ordered_attributes_keys=None, categorical_attributes_keys=[\"cell_type\", \"sex\", \"donor_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d86b5e-fc89-475a-8af6-7078ccd476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Biolord model\n",
    "\n",
    "module_params = {\n",
    "    \"decoder_width\": 1024,\n",
    "    \"decoder_depth\": 4,\n",
    "    \"attribute_nn_width\": 512,\n",
    "    \"attribute_nn_depth\": 2,\n",
    "    \"n_latent_attribute_categorical\": 4,\n",
    "    \"gene_likelihood\": \"normal\",\n",
    "    \"reconstruction_penalty\": 1e2,\n",
    "    \"unknown_attribute_penalty\": 1e1,\n",
    "    \"unknown_attribute_noise_param\": 1e-1,\n",
    "    \"attribute_dropout_rate\": 0.1,\n",
    "    \"use_batch_norm\": False,\n",
    "    \"use_layer_norm\": False,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b4dd4-cb42-442f-b3fd-f941b5b74b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = biolord.Biolord(\n",
    "    adata=adata_normal,\n",
    "    n_latent=32,\n",
    "    model_name=\"Healthy_Hamstring_Biolord_check_with_local_norm\",\n",
    "    module_params=module_params,\n",
    "    train_classifiers=False,\n",
    "    split_key=\"split\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896111d-2ea9-4a03-99af-24e9a5bf6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer_params = {\n",
    "    \"n_epochs_warmup\": 0,\n",
    "    \"latent_lr\": 1e-4,\n",
    "    \"latent_wd\": 1e-4,\n",
    "    \"decoder_lr\": 1e-4,\n",
    "    \"decoder_wd\": 1e-4,\n",
    "    \"attribute_nn_lr\": 1e-2,\n",
    "    \"attribute_nn_wd\": 4e-8,\n",
    "    \"step_size_lr\": 45,\n",
    "    \"cosine_scheduler\": True,\n",
    "    \"scheduler_final_lr\": 1e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473c7cb-2522-46d3-8ce1-cb74cd7ce266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    max_epochs=500,\n",
    "    batch_size=512,\n",
    "    plan_kwargs=trainer_params,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=20,\n",
    "    check_val_every_n_epoch=10,\n",
    "    num_workers=1,\n",
    "    enable_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14135-a9c2-495c-902b-88d62f8b172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/work/Biolord_all/Healthy_Hamstring_Biolord/Newest_Biolord_HH/Biolord_Healthy_Hamstring/HH_Biolord_test_raw_extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28513ce2-bcab-4a48-8c89-14bdbc742858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"/work/Biolord_all/Healthy_Hamstring_Biolord/Newest_Biolord_HH/Biolord_Healthy_Hamstring/HH_Biolord_test_raw_extra_lesscov\", adata = adata_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1d5b0-0d22-444c-860c-41d2dbcae3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0c4bc-71ce-4566-86e7-14931ebe6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recovery for all the data\n",
    "z_basal, z = model.get_latent_representation_adata(\n",
    "    adata_normal,\n",
    "    batch_size=256,\n",
    ")\n",
    "#rec, _ = model.predict(adata_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53a50a-0090-41e0-ae2f-17451be29977",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_residual = z[:, z_basal.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09b83f-b874-466e-890a-4f9256640bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786620b-c775-488b-b9e9-383ec361523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"z_basal shape:\", z_basal.X.shape) \n",
    "print(\"z shape:\", z.X.shape)             \n",
    "\n",
    "# Unknown: \n",
    "z_unknown = z.X[:, z_basal.X.shape[1]:]\n",
    "print(\"z_unknown shape:\", z_unknown.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1438c98-b074-427d-8300-d152975623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627455d8-e4f2-4071-b5d4-c46c9fbb22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091e4ea-61f6-470d-bfd7-b5f36ef0e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec, _ = model.predict(adata_normal[adata_normal.obs[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f1fb4-a3bb-4c67-a5f9-3b0e0b7f778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0b2ed-cb94-4449-85d0-46f7e8384e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the log1p \n",
    "rec.X = np.expm1(rec.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ae9ab-7a4a-49c0-874a-055207b9cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_norm_size = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23a881-0215-49d3-bb24-8454e515ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.X = (rec.X / true_norm_size) * adata_test.obs[\"total_counts\"].values[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed8e2c-854a-4450-b47a-4a780992eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test.var[\"total_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56262a-269d-46d8-ac4b-c8cdce9fd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the raw test data from the original unnormalized adata\n",
    "X_true = adata[adata.obs[\"split\"] == \"test\"].X  # From raw (not normalized) data\n",
    "rec_array = rec.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd194e8-d1ea-4945-9834-15df8becdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = adata.obs[[\"donor_id\", \"development_stage\", \"development_stage_ontology_term_id\"]].copy()\n",
    "# Check how many unique combinations exist\n",
    "unique_combinations = df.drop_duplicates()\n",
    "print(f\"Unique combinations:\\n{unique_combinations}\")\n",
    "# Check pairwise uniqueness\n",
    "for col1 in df.columns:\n",
    "    for col2 in df.columns:\n",
    "        if col1 != col2:\n",
    "            is_unique = df.groupby(col1)[col2].nunique().max() == 1\n",
    "            print(f\"{col1} → {col2} is one-to-one: {is_unique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a3580-67aa-43c2-9001-298d9649b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import scipy.sparse\n",
    "\n",
    "# Convert to dense\n",
    "if scipy.sparse.issparse(X_true):\n",
    "    X_true = X_true.toarray()\n",
    "if scipy.sparse.issparse(rec_array):\n",
    "    rec_array = rec_array.toarray()\n",
    "\n",
    "# Flatten\n",
    "X_true_flat = X_true.flatten()\n",
    "rec_flat = rec_array.flatten()\n",
    "\n",
    "# R² score\n",
    "r2_flat = r2_score(X_true_flat, rec_flat)\n",
    "print(\" Global R² (flattened):\", r2_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b67ed-bd2f-44d4-8c43-62c4496de0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE when test_adata is passed \n",
    "\n",
    "mse = mean_squared_error(X_true,rec_array)\n",
    "\n",
    "print(f\"MSE score with test_adata: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152ded2-9b51-47d1-80e8-1a6ba1bace0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE when test_adata is passed \n",
    "\n",
    "mae = mean_absolute_error(X_true, rec_array)\n",
    "\n",
    "print(f\"MAE score with test_adata: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df1dde-793c-4cc5-a73f-91c2bc12ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If rec is an AnnData object, extract the X attribute (i.e., the data matrix)\n",
    "import anndata\n",
    "if isinstance(rec, anndata.AnnData):\n",
    "    rec = rec.X\n",
    "\n",
    "# Now, rec should be a numpy array or sparse matrix, which is what obsm expects\n",
    "adata_test.obsm[\"X_reconstructed\"] = rec\n",
    "\n",
    "# Save the entire object with the reconstructed data\n",
    "adata_test.write(\"biolord_HH__fix_final_removed_batch_effect.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747c19f-d63c-47a5-bc15-8f32a37bef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Save original raw counts\n",
    "X_original = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5fd87-00e9-45d6-8d61-77e13e7979c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original total counts\n",
    "if scipy.sparse.issparse(X_original):\n",
    "    total_counts = np.array(X_original.sum(axis=1)).flatten()\n",
    "else:\n",
    "    total_counts = X_original.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ff835-0b19-4b5a-9b49-a99c60a2da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and log1p\n",
    "adata_tmp = adata.copy()\n",
    "sc.pp.normalize_total(adata_tmp, target_sum=1e4)\n",
    "sc.pp.log1p(adata_tmp)\n",
    "X_norm_log = adata_tmp.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609758d-60d6-4967-8895-00a4e348dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse log1p\n",
    "if scipy.sparse.issparse(X_norm_log):\n",
    "    X_reversed = np.expm1(X_norm_log.toarray())\n",
    "else:\n",
    "    X_reversed = np.expm1(X_norm_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2b2da-9f87-427a-82e3-b8e0fec8f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse normalization\n",
    "X_reconstructed = (X_reversed / 1e4) * total_counts[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0860e04-132d-48a1-af25-47a7f0601520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original to dense for comparison\n",
    "if scipy.sparse.issparse(X_original):\n",
    "    X_original_dense = X_original.toarray()\n",
    "else:\n",
    "    X_original_dense = X_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf8ae8-1bdc-4a26-acbf-bc82f619d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "mae = np.mean(np.abs(X_reconstructed - X_original_dense))\n",
    "max_diff = np.max(np.abs(X_reconstructed - X_original_dense))\n",
    "\n",
    "print(f\"Mean Absolute Error (round-trip): {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b15327-eee1-4e2e-a462-3f70ccbf79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "###MIG###\n",
    "import numpy as np\n",
    "import anndata\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical(data):\n",
    "    encoders = []\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "        encoders.append(le)\n",
    "    return encoded_data, encoders\n",
    "\n",
    "def prep_data(adata, embedding, covriate_keys=None):\n",
    "    encoded_factors_of_variation, _ = encode_categorical(adata.obs[covriate_keys].values)\n",
    "\n",
    "    if isinstance(embedding, anndata.AnnData):  \n",
    "        embedding_data = embedding.X\n",
    "    else:\n",
    "        embedding_data = embedding\n",
    "\n",
    "    mus = np.array(embedding_data)\n",
    "    ys = np.array(encoded_factors_of_variation)\n",
    "\n",
    "    return mus.T.copy(), ys.T.copy()\n",
    "\n",
    "\n",
    "def compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes the mutual information gap.\"\"\"\n",
    "    return _compute_mig(mus, ys, covariate_names)\n",
    "\n",
    "def _compute_mig(mus, ys, covariate_names=None):\n",
    "    \"\"\"Computes MIG score based on latent codes and covariates.\"\"\"\n",
    "    score_dict = {}\n",
    "    discretized_mus = make_discretizer(mus, discretizer_fn=_histogram_discretize)\n",
    "   # print(\"Sample Discretized Latent Variables:\\n\", discretized_mus[:, :5])\n",
    "    m = discrete_mutual_info(discretized_mus, ys)\n",
    "\n",
    "    if covariate_names is None:\n",
    "        covariate_names = [f\"Covariate {j}\" for j in range(m.shape[1])]\n",
    "        \n",
    "    for j in range(m.shape[1]):\n",
    "        top_indices = np.argsort(m[:, j])[::-1][:3]\n",
    "        top_scores = m[top_indices, j]\n",
    "        print(f\"Top 3 MI scores for covariate '{covariate_names[j]}':\")\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            print(f\"  Latent dim {idx}: MI = {score:.4f}\")\n",
    "\n",
    "    assert m.shape[0] == mus.shape[0]\n",
    "    assert m.shape[1] == ys.shape[0]\n",
    "\n",
    "    entropy = discrete_entropy(ys)\n",
    "    sorted_m = np.sort(m, axis=0)[::-1]\n",
    "\n",
    "    score_dict[\"discrete_mig\"] = np.mean(\n",
    "        np.divide(sorted_m[0, :] - sorted_m[1, :], entropy[:])\n",
    "    )\n",
    "\n",
    "    print(\"Þetta er score:\", score_dict)\n",
    "    print(\"Entropy values:\", entropy)\n",
    "    return score_dict\n",
    "\n",
    "def discrete_mutual_info(mus, ys):\n",
    "    num_codes = mus.shape[0]\n",
    "    num_factors = ys.shape[0]\n",
    "    m = np.zeros([num_codes, num_factors])\n",
    "    \n",
    "    for i in range(num_codes):\n",
    "        for j in range(num_factors):\n",
    "            m[i, j] = mutual_info_score(ys[j, :], mus[i, :])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def discrete_entropy(ys):\n",
    "    num_factors = ys.shape[0]\n",
    "    h = np.zeros(num_factors)\n",
    "    \n",
    "    for j in range(num_factors):\n",
    "        h[j] = mutual_info_score(ys[j, :], ys[j, :])\n",
    "    \n",
    "    return h\n",
    "\n",
    "def _identity_discretizer(target, num_bins):\n",
    "    del num_bins\n",
    "    return target\n",
    "\n",
    "\n",
    "def _histogram_discretize(target, num_bins=10):\n",
    "    discretized = np.zeros_like(target)\n",
    "    for i in range(target.shape[0]):\n",
    "        discretized[i, :] = np.digitize(target[i, :], np.histogram(\n",
    "            target[i, :], num_bins)[1][:-1])\n",
    "    return discretized\n",
    "\n",
    "\n",
    "def make_discretizer(target, num_bins=10, discretizer_fn=_histogram_discretize):\n",
    "    return discretizer_fn(target, num_bins)\n",
    "\n",
    "\n",
    "def score_disentanglement(adata, embedding_data, embedding_basal, covriate_keys=None, continuous_covriate_keys=None):\n",
    "    mus, ys = prep_data(adata, embedding_data, covriate_keys=covriate_keys)\n",
    "    print('Computing MIG')\n",
    "    mig = compute_mig(mus, ys, covariate_names=covriate_keys)\n",
    "    return mig, mus, ys\n",
    "\n",
    "# Run MIG score\n",
    "mig_1,mus,ys = score_disentanglement(\n",
    "    adata_normal,\n",
    "    z,\n",
    "    None,\n",
    "    covriate_keys=[\"cell_type\", \"sex\", \"donor_id\", \"development_stage_ontology_term_id\", \"development_stage\"]\n",
    ")\n",
    "\n",
    "print(\"MIG Score:\", mig_1)\n",
    "print(\"Latent variances:\", np.var(mus, axis=1))\n",
    "\n",
    "discretized_mus = _histogram_discretize(mus)\n",
    "m = discrete_mutual_info(discretized_mus, ys)\n",
    "\n",
    "print(\"MI matrix shape:\", m.shape)\n",
    "print(\"Max MI per factor:\", np.max(m, axis=0))\n",
    "print(\"Which latents have highest MI per factor:\", np.argmax(m, axis=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd23a6-6200-4599-8c4b-a0cfabe300f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covriate_keys = [\"cell_type\", \"sex\", \"donor_id\", \"development_stage_ontology_term_id\", \"development_stage\"]\n",
    "for key in covriate_keys:\n",
    "    print(f\"{key} — unique values:\")\n",
    "    print(adata_normal.obs[key].unique())\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9555c7b-9dc2-4568-8e51-4e6f920330fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized DCI computation based on disentanglement_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Encoding and Preprocessing ===\n",
    "def encode_categorical(data):\n",
    "    encoded_data = np.zeros_like(data, dtype=int)\n",
    "    for i in range(data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, i] = le.fit_transform(data[:, i])\n",
    "    return encoded_data\n",
    "\n",
    "def remove_duplicate_columns(df):\n",
    "    df_unique = df.T.drop_duplicates().T\n",
    "    return df_unique\n",
    "\n",
    "def prep_data(adata, embedding, covariate_keys, test_size=0.25):\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        range(len(adata)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    cov_df = adata.obs[covariate_keys].copy()\n",
    "    cov_df = remove_duplicate_columns(cov_df)\n",
    "    encoded_factors = encode_categorical(cov_df.values)\n",
    "    embedding_data = embedding.X if isinstance(embedding, anndata.AnnData) else embedding\n",
    "    mus_train = embedding_data[idx_train]\n",
    "    mus_test = embedding_data[idx_test]\n",
    "    ys_train = encoded_factors[idx_train]\n",
    "    ys_test = encoded_factors[idx_test]\n",
    "    return mus_train.T, ys_train.T, mus_test.T, ys_test.T\n",
    "\n",
    "# === Importance Matrix ===\n",
    "def compute_importance_rf(x_train, y_train, x_test, y_test):\n",
    "    num_factors = y_train.shape[0]\n",
    "    num_codes = x_train.shape[0]\n",
    "    importance_matrix = np.zeros((num_codes, num_factors))\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(num_factors):\n",
    "        model = RandomForestClassifier(random_state=42, max_depth=5)\n",
    "        model.fit(x_train.T, y_train[i])\n",
    "        importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "        train_acc.append(np.mean(model.predict(x_train.T) == y_train[i]))\n",
    "        test_acc.append(np.mean(model.predict(x_test.T) == y_test[i]))\n",
    "    return importance_matrix, np.mean(train_acc), np.mean(test_acc)\n",
    "\n",
    "# === Disentanglement ===\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "    row_sums = importance_matrix.sum(axis=1, keepdims=True)\n",
    "    safe_matrix = np.where(row_sums == 0, 1e-11, row_sums)\n",
    "    normalized = importance_matrix / safe_matrix\n",
    "    return 1. - entropy(normalized.T + 1e-11, base=importance_matrix.shape[1])\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "    per_code = disentanglement_per_code(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    code_importance = importance_matrix.sum(axis=1) / total\n",
    "    return np.sum(per_code * code_importance)\n",
    "\n",
    "# === Completeness ===\n",
    "def completeness_per_factor(importance_matrix):\n",
    "    return 1. - entropy(importance_matrix + 1e-11, base=importance_matrix.shape[0])\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "    per_factor = completeness_per_factor(importance_matrix)\n",
    "    total = importance_matrix.sum()\n",
    "    if total == 0.:\n",
    "        return 0.0\n",
    "    factor_importance = importance_matrix.sum(axis=0) / total\n",
    "    return np.sum(per_factor * factor_importance)\n",
    "\n",
    "# === DCI Master Function ===\n",
    "def compute_dci(mus_train, ys_train, mus_test, ys_test):\n",
    "    importance_matrix, train_acc, test_acc = compute_importance_rf(\n",
    "        mus_train, ys_train, mus_test, ys_test\n",
    "    )\n",
    "    threshold = 1e-11\n",
    "    importance_matrix = np.where(importance_matrix < threshold, 0, importance_matrix)\n",
    "    return {\n",
    "    \"disentanglement\": disentanglement(importance_matrix),\n",
    "    \"completeness\": completeness(importance_matrix),\n",
    "    \"importance_matrix\": importance_matrix,\n",
    "    \"train_acc\": train_acc,\n",
    "    \"test_acc\": test_acc,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c089357-c8b7-4574-8764-d484a8581741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covariate_keys = [\"cell_type\", \"sex\", \"donor_id\"]\n",
    "mus_train, ys_train, mus_test, ys_test = prep_data(adata, z, covariate_keys)\n",
    "dci_scores = compute_dci(mus_train, ys_train, mus_test, ys_test)\n",
    "print(dci_scores)\n",
    "\n",
    "importance_matrix = dci_scores[\"importance_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe994dc-cca8-427f-8261-bd963ef5e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"\n",
    "    Computes the SAP score.\n",
    "\n",
    "    Args:\n",
    "        mus: (latent_dim, num_samples) training representations\n",
    "        ys: (num_factors, num_samples) training factors\n",
    "        mus_test: (latent_dim, num_samples) test representations\n",
    "        ys_test: (num_factors, num_samples) test factors\n",
    "        continuous_factors: Whether the factors are continuous\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with SAP score.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing SAP score...\")\n",
    "    _validate_shapes(mus, ys, mus_test, ys_test)\n",
    "\n",
    "    score_matrix = compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors)\n",
    "    scores_dict = {}\n",
    "    scores_dict[\"SAP_score\"] = compute_avg_diff_top_two(score_matrix)\n",
    "    logging.info(\"SAP score: %.4f\", scores_dict[\"SAP_score\"])\n",
    "    return scores_dicte\n",
    "\n",
    "\n",
    "def compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "    \"\"\"Compute score matrix for each latent and factor combination.\"\"\"\n",
    "    num_latents, num_samples = mus.shape\n",
    "    num_factors = ys.shape[0]\n",
    "\n",
    "    score_matrix = np.zeros([num_latents, num_factors])\n",
    "    for i in range(num_latents):\n",
    "        for j in range(num_factors):\n",
    "            mu_i = mus[i, :]\n",
    "            y_j = ys[j, :]\n",
    "\n",
    "            if continuous_factors:\n",
    "                cov_mu_i_y_j = np.cov(mu_i, y_j, ddof=1)\n",
    "                cov_mu_y = cov_mu_i_y_j[0, 1] ** 2\n",
    "                var_mu = cov_mu_i_y_j[0, 0]\n",
    "                var_y = cov_mu_i_y_j[1, 1]\n",
    "                if var_mu > 1e-12 and var_y > 1e-12:\n",
    "                    score_matrix[i, j] = cov_mu_y / (var_mu * var_y)\n",
    "                else:\n",
    "                    score_matrix[i, j] = 0.0\n",
    "            else:\n",
    "                mu_i_test = mus_test[i, :]\n",
    "                y_j_test = ys_test[j, :]\n",
    "                classifier = svm.LinearSVC(C=0.01, class_weight=\"balanced\", max_iter=10000)\n",
    "                classifier.fit(mu_i[:, np.newaxis], y_j)\n",
    "                pred = classifier.predict(mu_i_test[:, np.newaxis])\n",
    "                score_matrix[i, j] = np.mean(pred == y_j_test)\n",
    "\n",
    "    return score_matrix\n",
    "\n",
    "\n",
    "def compute_avg_diff_top_two(score_matrix):\n",
    "    \"\"\"Compute average difference between top two latent scores per factor.\"\"\"\n",
    "    sorted_matrix = np.sort(score_matrix, axis=0)\n",
    "    return np.mean(sorted_matrix[-1, :] - sorted_matrix[-2, :])\n",
    "\n",
    "\n",
    "def _validate_shapes(mus, ys, mus_test, ys_test):\n",
    "    \"\"\"Ensure shapes are valid for SAP computation.\"\"\"\n",
    "    assert mus.shape[1] == ys.shape[1], \"Mismatch: mus and ys sample count\"\n",
    "    assert mus_test.shape[1] == ys_test.shape[1], \"Mismatch: mus_test and ys_test sample count\"\n",
    "    assert mus.shape[0] == mus_test.shape[0], \"Mismatch in latent dimensions\"\n",
    "    assert ys.shape[0] == ys_test.shape[0], \"Mismatch in number of factors\"\n",
    "\n",
    "\n",
    "\n",
    "# mus, ys: (latent_dim, num_samples)\n",
    "# mus_test, ys_test: same shapes\n",
    "sap_score = compute_sap(mus_train, ys_train, mus_test, ys_test, continuous_factors=False)\n",
    "print(\"SAP:\", sap_score[\"SAP_score\"])\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_sap_score_matrix(score_matrix, factor_names=None, latent_names=None, title=\"SAP Score Matrix\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(score_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar_kws={\"label\": \"Score\"})\n",
    "    \n",
    "    plt.xlabel(\"Factors\")\n",
    "    plt.ylabel(\"Latent Dimensions\")\n",
    "    if factor_names:\n",
    "        plt.xticks(ticks=np.arange(len(factor_names)) + 0.5, labels=factor_names, rotation=45, ha=\"right\")\n",
    "    if latent_names:\n",
    "        plt.yticks(ticks=np.arange(len(latent_names)) + 0.5, labels=latent_names, rotation=0)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "score_matrix = compute_score_matrix(mus_train, ys_train, mus_test, ys_test, continuous_factors=False)\n",
    "plot_sap_score_matrix(score_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c967f-53a4-41bb-a993-1c726212f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRS \n",
    "\n",
    "\n",
    "def compute_irs(mus, ys, diff_quantile=0.99):\n",
    "    ys_discrete = make_discretizer(ys)\n",
    "\n",
    "    active_mask = (mus.var(axis=1) > 0)\n",
    "    active_mus = mus[active_mask, :]\n",
    "\n",
    "    if active_mus.size == 0:\n",
    "        irs_score = 0.0\n",
    "    else:\n",
    "        irs_score = scalable_disentanglement_score(ys_discrete.T, active_mus.T, diff_quantile)[\"avg_score\"]\n",
    "\n",
    "    score_dict = {}\n",
    "    score_dict[\"IRS\"] = irs_score\n",
    "    score_dict[\"num_active_dims\"] = int(np.sum(active_mask))\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def _drop_constant_dims(ys):\n",
    "    \"\"\"Returns a view of the matrix `ys` with dropped constant rows.\"\"\"\n",
    "    ys = np.asarray(ys)\n",
    "    if ys.ndim != 2:\n",
    "        raise ValueError(\"Expecting a matrix.\")\n",
    "\n",
    "    variances = ys.var(axis=1)\n",
    "    active_mask = variances > 0.\n",
    "    return ys[active_mask, :]\n",
    "\n",
    "\n",
    "def scalable_disentanglement_score(gen_factors, latents, diff_quantile=0.99):\n",
    "    \"\"\"Computes IRS scores of a dataset.\n",
    "\n",
    "    Assumes no noise in X and crossed generative factors (i.e. one sample per\n",
    "    combination of gen_factors). Assumes each g_i is an equally probable\n",
    "    realization of g_i and all g_i are independent.\n",
    "\n",
    "    Args:\n",
    "        gen_factors: Numpy array of shape (num samples, num generative factors),\n",
    "            matrix of ground truth generative factors.\n",
    "        latents: Numpy array of shape (num samples, num latent dimensions), matrix\n",
    "            of latent variables.\n",
    "        diff_quantile: Float value between 0 and 1 to decide what quantile of diffs\n",
    "            to select (use 1.0 for the version in the paper).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with IRS scores.\n",
    "    \"\"\"\n",
    "    num_gen = gen_factors.shape[1]\n",
    "    num_lat = latents.shape[1]\n",
    "\n",
    "    # Compute normalizer.\n",
    "    max_deviations = np.max(np.abs(latents - latents.mean(axis=0)), axis=0)\n",
    "    cum_deviations = np.zeros([num_lat, num_gen])\n",
    "    for i in range(num_gen):\n",
    "        unique_factors = np.unique(gen_factors[:, i], axis=0)\n",
    "        assert unique_factors.ndim == 1\n",
    "        num_distinct_factors = unique_factors.shape[0]\n",
    "        for k in range(num_distinct_factors):\n",
    "            # Compute E[Z | g_i].\n",
    "            match = gen_factors[:, i] == unique_factors[k]\n",
    "            e_loc = np.mean(latents[match, :], axis=0)\n",
    "\n",
    "            # Difference of each value within that group of constant g_i to its mean.\n",
    "            diffs = np.abs(latents[match, :] - e_loc)\n",
    "            max_diffs = np.percentile(diffs, q=diff_quantile*100, axis=0)\n",
    "            cum_deviations[:, i] += max_diffs\n",
    "        cum_deviations[:, i] /= num_distinct_factors\n",
    "    # Normalize value of each latent dimension with its maximal deviation.\n",
    "    normalized_deviations = cum_deviations / max_deviations[:, np.newaxis]\n",
    "    irs_matrix = 1.0 - normalized_deviations\n",
    "    disentanglement_scores = irs_matrix.max(axis=1)\n",
    "    if np.sum(max_deviations) > 0.0:\n",
    "        avg_score = np.average(disentanglement_scores, weights=max_deviations)\n",
    "    else:\n",
    "        avg_score = np.mean(disentanglement_scores)\n",
    "\n",
    "    parents = irs_matrix.argmax(axis=1)\n",
    "    score_dict = {}\n",
    "    score_dict[\"disentanglement_scores\"] = disentanglement_scores\n",
    "    score_dict[\"avg_score\"] = avg_score\n",
    "    score_dict[\"parents\"] = parents\n",
    "    score_dict[\"IRS_matrix\"] = irs_matrix\n",
    "    score_dict[\"max_deviations\"] = max_deviations\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "\n",
    "irs = compute_irs(mus_train, ys_train, diff_quantile=0.99)\n",
    "irs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18218e65-58b1-482d-9341-168a12f83109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(importance_matrix, cmap=\"viridis\", annot=False)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"Latent dimensions\")\n",
    "plt.title(\"DCI Importance Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
